{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_CACHE\"] = \"/tmp/wendler/hf_cache\"\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../NewtonRaphsonInversion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"/share/datasets/datasets/laicoyo/{000000..000009}.tar\"\n",
    "save_path = \"/data_shared/wendler/turbo_latents_from_laicoyo_inversion\"\n",
    "blocks_to_save = [\n",
    "        'unet.down_blocks.2.attentions.1',\n",
    "        'unet.mid_block.attentions.0',\n",
    "        'unet.up_blocks.0.attentions.0',\n",
    "        'unet.up_blocks.0.attentions.1',\n",
    "    ]\n",
    "n_max = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import logging\n",
    "import torch\n",
    "import io\n",
    "\n",
    "def url_to_dataloader(url, num_workers=4, batch_size=16, shuffle=False):\n",
    "    def log_and_continue(exn):\n",
    "        \"\"\"Call in an exception handler to ignore any exception, issue a warning, and continue.\"\"\"\n",
    "        logging.warning(f'Handling webdataset error ({repr(exn)}). Ignoring.')\n",
    "        return True\n",
    "    \n",
    "    def filter_no_latent(sample):\n",
    "        return 'latent.pt' in sample\n",
    "\n",
    "    def load_latent(z):\n",
    "        return torch.load(io.BytesIO(z), map_location='cpu').to(torch.float32)\n",
    "    if shuffle:\n",
    "        pipeline = [\n",
    "            wds.SimpleShardList(url),\n",
    "            wds.split_by_node,\n",
    "            wds.split_by_worker,\n",
    "            wds.tarfile_to_samples(handler=log_and_continue),\n",
    "            wds.select(filter_no_latent),\n",
    "            wds.shuffle(bufsize=5000, initial=1000),\n",
    "            wds.rename(image=\"latent.pt\", txt=\"txt\"),\n",
    "            wds.map_dict(image=load_latent, txt=lambda x: x.decode(\"utf-8\")),\n",
    "            wds.to_tuple(\"image\", \"txt\"),\n",
    "            wds.batched(batch_size, partial=False),\n",
    "        ]\n",
    "    else:\n",
    "        pipeline = [\n",
    "            wds.SimpleShardList(url),\n",
    "            wds.split_by_node,\n",
    "            wds.split_by_worker,\n",
    "            wds.tarfile_to_samples(handler=log_and_continue),\n",
    "            wds.select(filter_no_latent),\n",
    "            wds.rename(image=\"latent.pt\", txt=\"txt\"),\n",
    "            wds.map_dict(image=load_latent, txt=lambda x: x.decode(\"utf-8\")),\n",
    "            wds.to_tuple(\"image\", \"txt\"),\n",
    "            wds.batched(batch_size, partial=False),\n",
    "        ]\n",
    "\n",
    "    dataset = wds.DataPipeline(*pipeline)\n",
    "\n",
    "    loader = wds.WebLoader(\n",
    "        dataset, batch_size=None, shuffle=False, num_workers=num_workers,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = url_to_dataloader(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was a problem when trying to write in your cache folder (/share/u/models/hub). Please, ensure the directory exists and can be written to.\n",
      "/share/u/wendler/.conda/envs/sdxlsae/lib/python3.12/site-packages/diffusers/models/vq_model.py:20: FutureWarning: `VQEncoderOutput` is deprecated and will be removed in version 0.31. Importing `VQEncoderOutput` from `diffusers.models.vq_model` is deprecated and this will be removed in a future version. Please use `from diffusers.models.autoencoders.vq_model import VQEncoderOutput`, instead.\n",
      "  deprecate(\"VQEncoderOutput\", \"0.31\", deprecation_message)\n",
      "/share/u/wendler/.conda/envs/sdxlsae/lib/python3.12/site-packages/diffusers/models/vq_model.py:25: FutureWarning: `VQModel` is deprecated and will be removed in version 0.31. Importing `VQModel` from `diffusers.models.vq_model` is deprecated and this will be removed in a future version. Please use `from diffusers.models.autoencoders.vq_model import VQModel`, instead.\n",
      "  deprecate(\"VQModel\", \"0.31\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "from SDLens import HookedStableDiffusionXLImg2ImgPipeline\n",
    "from src.config import RunConfig\n",
    "from ipywidgets import Text, VBox\n",
    "import PIL\n",
    "from src.euler_scheduler import MyEulerAncestralDiscreteScheduler\n",
    "from diffusers.pipelines.auto_pipeline import AutoPipelineForImage2Image\n",
    "from src.sdxl_inversion_pipeline import SDXLDDIMPipeline\n",
    "from PIL import Image\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from IPython.display import display\n",
    "\n",
    "model = \"stabilityai/sdxl-turbo\"\n",
    "\n",
    "def inversion_callback(pipe, step, timestep, callback_kwargs):\n",
    "    return callback_kwargs\n",
    "\n",
    "\n",
    "def inference_callback(pipe, step, timestep, callback_kwargs):\n",
    "    return callback_kwargs\n",
    "\n",
    "class ImageEditorDemo:\n",
    "    def __init__(self, pipe_inversion, pipe_inference, latents, prompts, cfg, edit_cfg=1.2):\n",
    "        self.pipe_inversion = pipe_inversion\n",
    "        self.pipe_inference = pipe_inference\n",
    "        self.load_image = True\n",
    "        g_cpu = torch.Generator().manual_seed(7865)\n",
    "        if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "            img_size = (1024,1024)\n",
    "        else:\n",
    "            img_size = (512,512)\n",
    "        # resise input image\n",
    "        VQAE_SCALE = 8\n",
    "        latents_size = (1, 4, img_size[0] // VQAE_SCALE, img_size[1] // VQAE_SCALE)\n",
    "        latents_size = latents.shape\n",
    "        noise = [randn_tensor(latents_size, dtype=pipe_inversion.unet.dtype, device=torch.device(\"cuda:0\"), generator=g_cpu) for i\n",
    "                 in range(cfg.num_inversion_steps)]\n",
    "        pipe_inversion.scheduler.set_noise_list(noise)\n",
    "        pipe_inference.scheduler.set_noise_list(noise)\n",
    "        pipe_inversion.scheduler_inference.set_noise_list(noise)\n",
    "        pipe_inversion.set_progress_bar_config(disable=True)\n",
    "        pipe_inference.set_progress_bar_config(disable=True)\n",
    "        self.cfg = cfg\n",
    "        self.pipe_inversion.cfg = cfg\n",
    "        self.pipe_inference.cfg = cfg\n",
    "        self.inv_hp = [2, 0.1, 0.2] # niter, alpha, lr 2, 0.1, 0.2 is default\n",
    "        self.edit_cfg = edit_cfg\n",
    "        self.latents = latents\n",
    "        self.last_latent = self.invert(latents, prompts)\n",
    "        self.original_latent = self.last_latent\n",
    "\n",
    "    def invert(self, latents, base_prompts):\n",
    "        res = self.pipe_inversion.invert_latents(prompt=base_prompts,\n",
    "                             num_inversion_steps=self.cfg.num_inversion_steps,\n",
    "                             num_inference_steps=self.cfg.num_inference_steps,\n",
    "                             latents=latents,\n",
    "                             guidance_scale=self.cfg.guidance_scale,\n",
    "                             callback_on_step_end=inversion_callback,\n",
    "                             strength=self.cfg.inversion_max_step,\n",
    "                             denoising_start=1.0 - self.cfg.inversion_max_step,\n",
    "                             inv_hp=self.inv_hp)[0][0]\n",
    "        return res\n",
    "\n",
    "    def edit(self, target_prompt, guidance_scale=None):\n",
    "        if guidance_scale is None:\n",
    "            guidance_scale = self.edit_cfg\n",
    "        image = self.pipe_inference(prompt=target_prompt,\n",
    "                            num_inference_steps=self.cfg.num_inference_steps,\n",
    "                            negative_prompt=\"\",\n",
    "                            callback_on_step_end=inference_callback,\n",
    "                            image=self.last_latent,\n",
    "                            strength=self.cfg.inversion_max_step,\n",
    "                            denoising_start=1.0 - self.cfg.inversion_max_step,\n",
    "                            guidance_scale=guidance_scale).images[0]\n",
    "        return image.resize((512, 512))\n",
    "    \n",
    "    def edit_and_cache(self, target_prompt, guidance_scale=None):\n",
    "        if guidance_scale is None:\n",
    "            guidance_scale = self.edit_cfg\n",
    "        image, cache = self.pipe_inference.run_with_cache(prompt=target_prompt,\n",
    "                            positions_to_cache=blocks_to_save,\n",
    "                            save_input=True,\n",
    "                            save_output=True,\n",
    "                            output_type='pil',\n",
    "                            num_inference_steps=self.cfg.num_inference_steps,\n",
    "                            negative_prompt=\"\",\n",
    "                            callback_on_step_end=inference_callback,\n",
    "                            image=self.last_latent,\n",
    "                            strength=self.cfg.inversion_max_step,\n",
    "                            denoising_start=1.0 - self.cfg.inversion_max_step,\n",
    "                            guidance_scale=guidance_scale)\n",
    "        return image, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "    image_size = (1024,1024)\n",
    "    config = RunConfig(num_inference_steps=20,\n",
    "                   num_inversion_steps=20,\n",
    "                   guidance_scale=0.0,\n",
    "                   inversion_max_step=0.75) #4,4,0,0.6 is default settings 0.6 and 0.7 look the same\n",
    "else:\n",
    "    image_size = (512,512)\n",
    "    config = RunConfig(num_inference_steps=4,\n",
    "                   num_inversion_steps=4,\n",
    "                   guidance_scale=0.0,\n",
    "                   inversion_max_step=0.75) #4,4,0,0.6 is default settings 0.6 and 0.7 look the same\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'safety_checker': None} are not expected by SDXLDDIMPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c33fab5f864eba9fa615867ec9f636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'safety_checker': None} are not expected by StableDiffusionXLImg2ImgPipeline and will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea45093dca9140ff9be1c82b53c56e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "dtype = torch.float32\n",
    "scheduler_class = MyEulerAncestralDiscreteScheduler\n",
    "if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "    pipe_inversion = SDXLDDIMPipeline.from_pretrained(model, \n",
    "                                                      torch_dtype=dtype,\n",
    "                                                      device_map=\"balanced\",\n",
    "                                                      variant=(\"fp16\" if dtype==torch.float16 else None))\n",
    "    \n",
    "    pipe_inference = HookedStableDiffusionXLImg2ImgPipeline.from_pretrained(model, \n",
    "                                                                        torch_dtype=dtype,\n",
    "                                                                        device_map=\"balanced\",\n",
    "                                                                        variant=(\"fp16\" if dtype==torch.float16 else None)\n",
    "                                                                    )\n",
    "    if dtype == torch.float32:\n",
    "        pipe_inversion.text_encoder_2.to(dtype)\n",
    "        pipe_inference.text_encoder_2.to(dtype)\n",
    "else:\n",
    "    pipe_inversion = SDXLDDIMPipeline.from_pretrained(model, use_safetensors=True, safety_checker=None, cache_dir=os.environ[\"HF_HOME\"]).to(device)\n",
    "    pipe_inference = HookedStableDiffusionXLImg2ImgPipeline.from_pretrained(model, use_safetensors=True, safety_checker=None, cache_dir=os.environ[\"HF_HOME\"]).to(device)\n",
    "\n",
    "#pipe_inference = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sdxl-turbo\", use_safetensors=True, safety_checker= None, cache_dir=os.environ[\"HF_HOME\"]).to(device)\n",
    "pipe_inference.scheduler            = scheduler_class.from_config(pipe_inference.scheduler.config)\n",
    "pipe_inversion.scheduler            = scheduler_class.from_config(pipe_inversion.scheduler.config)\n",
    "pipe_inversion.scheduler_inference  = scheduler_class.from_config(pipe_inference.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # it's bit more involved to align the 1 step and 4 step process \n",
    "    # the problem is that when not starting from 999 it is hard to align the scheduler timesteps (via its methods and pipeline args) between the two processes\n",
    "    # that's why i need to set: self_inversion_max_step = 1. in order to get error 0 between the latents\n",
    "    from copy import deepcopy\n",
    "    idx = 0\n",
    "    latents, prompts = next(iter(loader))\n",
    "    latents = latents.cuda()\n",
    "    latents *= pipe_inference.vae.config.scaling_factor\n",
    "    editor1 = ImageEditorDemo(pipe_inversion, pipe_inference, latents[idx].unsqueeze(0), prompts[idx], config, edit_cfg=0.0) \n",
    "    editor1.cfg.num_inference_steps = 4\n",
    "    editor1.cfg.inversion_max_step = 1.\n",
    "    img1, cache1 = editor1.edit_and_cache(\"a black lady with a huge afro \" + prompts[idx], guidance_scale=0.0)\n",
    "    print(pipe_inference.scheduler.timesteps)\n",
    "    editor1.cfg.num_inference_steps = 1\n",
    "    editor1.cfg.inversion_max_step = 1.\n",
    "    img2, cache2 = editor1.edit_and_cache(\"a black lady with a huge afro \" + prompts[idx], guidance_scale=0.0)\n",
    "    print(pipe_inference.scheduler.timesteps)\n",
    "    import matplotlib.pyplot as plt\n",
    "    print(prompts[idx])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img1[0][0])\n",
    "    plt.title('Image 1')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(img2[0][0])\n",
    "    plt.title('Image 2')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    for b in blocks_to_save:\n",
    "        print(b)\n",
    "        print(cache1['output'][b].shape)\n",
    "        print(cache2['output'][b].shape)\n",
    "        print((cache1['output'][b][:,0] - cache2['output'][b][:,0]).abs().mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512, 512, 3)\n",
      "(1, 512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "ct = datetime.datetime.now()\n",
    "save_path = os.path.join(save_path, str(ct))\n",
    "# Collecting dataset\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "writers = {\n",
    "    block: wds.TarWriter(f'{save_path}/{block}.tar') for block in blocks_to_save\n",
    "}\n",
    "\n",
    "writers.update({'images': wds.TarWriter(f'{save_path}/images.tar')})\n",
    "\n",
    "num_document = 0\n",
    "for d in loader:\n",
    "    latents, prompts = d\n",
    "    latents = latents.cuda()\n",
    "    latents *= pipe_inference.vae.config.scaling_factor\n",
    "    for latent, prompt in zip(latents, prompts):\n",
    "        config = RunConfig(num_inference_steps=4,\n",
    "                num_inversion_steps=4,\n",
    "                guidance_scale=0.0,\n",
    "                inversion_max_step=0.75)\n",
    "        editor = ImageEditorDemo(pipe_inversion, pipe_inference, latent.unsqueeze(0), prompt, config, edit_cfg=0.0) \n",
    "        with torch.no_grad():\n",
    "            output, cache = editor.edit_and_cache(prompt, guidance_scale=0.0)\n",
    "        blocks = cache['input'].keys()\n",
    "        for block in blocks:\n",
    "            sample = {\n",
    "                \"__key__\": f\"sample_{num_document}\",\n",
    "                \"output.pth\": cache['output'][block].cpu(),\n",
    "                \"diff.pth\": (cache['output'][block] - cache['input'][block]).cpu(),\n",
    "                \"prompt\": prompt,\n",
    "                \"inverted_latent.pth\": editor.last_latent.cpu(),\n",
    "                \"original_latent.pth\": latent.cpu(),\n",
    "                \"gen_args.json\": {\"num_inference_steps\": 4, \n",
    "                                \"inversion_max_step\": 0.75, \n",
    "                                \"guidance_scale\": 0.0, \n",
    "                                \"num_inversion_steps\": 4,\n",
    "                                \"edit_cfg\": 0.0}\n",
    "            }\n",
    "\n",
    "            writers[block].write(sample)\n",
    "            writers['images'].write({\n",
    "                \"__key__\": f\"sample_{num_document}\",\n",
    "                \"images.npy\": np.stack(output.images)\n",
    "            })\n",
    "        num_document += 1\n",
    "        if num_document >= n_max:\n",
    "            break\n",
    "    if num_document >= n_max:\n",
    "        break\n",
    "\n",
    "for block, writer in writers.items():\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxlsae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
