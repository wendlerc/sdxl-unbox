{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"HF_CACHE\"] = \"/tmp/wendler/hf_cache\"\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../NewtonRaphsonInversion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import logging\n",
    "import torch\n",
    "import io\n",
    "\n",
    "def url_to_dataloader(url, num_workers=4, batch_size=16):\n",
    "    def log_and_continue(exn):\n",
    "        \"\"\"Call in an exception handler to ignore any exception, issue a warning, and continue.\"\"\"\n",
    "        logging.warning(f'Handling webdataset error ({repr(exn)}). Ignoring.')\n",
    "        return True\n",
    "    \n",
    "    def filter_no_latent(sample):\n",
    "        return 'latent.pt' in sample\n",
    "\n",
    "    def load_latent(z):\n",
    "        return torch.load(io.BytesIO(z), map_location='cpu').to(torch.float32)\n",
    "    \n",
    "    pipeline = [\n",
    "        wds.SimpleShardList(url),\n",
    "        wds.split_by_node,\n",
    "        wds.split_by_worker,\n",
    "        wds.tarfile_to_samples(handler=log_and_continue),\n",
    "        wds.select(filter_no_latent),\n",
    "        wds.shuffle(bufsize=5000, initial=1000),\n",
    "        wds.rename(image=\"latent.pt\", txt=\"txt\"),\n",
    "        wds.map_dict(image=load_latent, txt=lambda x: x.decode(\"utf-8\")),\n",
    "        wds.to_tuple(\"image\", \"txt\"),\n",
    "        wds.batched(batch_size, partial=False),\n",
    "    ]\n",
    "\n",
    "    dataset = wds.DataPipeline(*pipeline)\n",
    "\n",
    "    loader = wds.WebLoader(\n",
    "        dataset, batch_size=None, shuffle=False, num_workers=num_workers,\n",
    "    )\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"/share/datasets/datasets/laicoyo/{000000..000009}.tar\"\n",
    "loader = url_to_dataloader(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = next(iter(loader))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sdxl_inversion_pipeline import SDXLDDIMPipeline\n",
    "from main import ImageEditorDemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SDLens import HookedStableDiffusionXLImg2ImgPipeline\n",
    "from src.config import RunConfig\n",
    "from ipywidgets import Text, VBox\n",
    "import PIL\n",
    "from src.euler_scheduler import MyEulerAncestralDiscreteScheduler\n",
    "from diffusers.pipelines.auto_pipeline import AutoPipelineForImage2Image\n",
    "from src.sdxl_inversion_pipeline import SDXLDDIMPipeline\n",
    "from PIL import Image\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from IPython.display import display\n",
    "\n",
    "model = \"stabilityai/sdxl-turbo\"\n",
    "\n",
    "def inversion_callback(pipe, step, timestep, callback_kwargs):\n",
    "    return callback_kwargs\n",
    "\n",
    "\n",
    "def inference_callback(pipe, step, timestep, callback_kwargs):\n",
    "    return callback_kwargs\n",
    "\n",
    "class ImageEditorDemo:\n",
    "    def __init__(self, pipe_inversion, pipe_inference, latents, prompts, cfg, edit_cfg=1.2):\n",
    "        self.pipe_inversion = pipe_inversion\n",
    "        self.pipe_inference = pipe_inference\n",
    "        self.load_image = True\n",
    "        g_cpu = torch.Generator().manual_seed(7865)\n",
    "        if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "            img_size = (1024,1024)\n",
    "        else:\n",
    "            img_size = (512,512)\n",
    "        # resise input image\n",
    "        VQAE_SCALE = 8\n",
    "        latents_size = (1, 4, img_size[0] // VQAE_SCALE, img_size[1] // VQAE_SCALE)\n",
    "        print(pipe_inversion.unet.dtype)\n",
    "        noise = [randn_tensor(latents_size, dtype=pipe_inversion.unet.dtype, device=torch.device(\"cuda:0\"), generator=g_cpu) for i\n",
    "                 in range(cfg.num_inversion_steps)]\n",
    "        print(noise[0].shape)\n",
    "        pipe_inversion.scheduler.set_noise_list(noise)\n",
    "        pipe_inference.scheduler.set_noise_list(noise)\n",
    "        pipe_inversion.scheduler_inference.set_noise_list(noise)\n",
    "        pipe_inversion.set_progress_bar_config(disable=True)\n",
    "        pipe_inference.set_progress_bar_config(disable=True)\n",
    "        self.cfg = cfg\n",
    "        self.pipe_inversion.cfg = cfg\n",
    "        self.pipe_inference.cfg = cfg\n",
    "        self.inv_hp = [2, 0.1, 0.2] # niter, alpha, lr 2, 0.1, 0.2 is default\n",
    "        self.edit_cfg = edit_cfg\n",
    "\n",
    "        #self.pipe_inference.to(\"cuda\")\n",
    "        #self.pipe_inversion.to(\"cuda\")\n",
    "        self.latents = latents\n",
    "        self.last_latent = self.invert(latents, prompts)\n",
    "        self.original_latent = self.last_latent\n",
    "\n",
    "    def invert(self, latents, base_prompts):\n",
    "        res = self.pipe_inversion.invert_latents(prompt=base_prompts,\n",
    "                             num_inversion_steps=self.cfg.num_inversion_steps,\n",
    "                             num_inference_steps=self.cfg.num_inference_steps,\n",
    "                             latents=latents,\n",
    "                             guidance_scale=self.cfg.guidance_scale,\n",
    "                             callback_on_step_end=inversion_callback,\n",
    "                             strength=self.cfg.inversion_max_step,\n",
    "                             denoising_start=1.0 - self.cfg.inversion_max_step,\n",
    "                             inv_hp=self.inv_hp)[0][0]\n",
    "        return res\n",
    "\n",
    "    def edit(self, target_prompt, guidance_scale=None):\n",
    "        if guidance_scale is None:\n",
    "            guidance_scale = self.edit_cfg\n",
    "        image = self.pipe_inference(prompt=target_prompt,\n",
    "                            num_inference_steps=self.cfg.num_inference_steps,\n",
    "                            negative_prompt=\"\",\n",
    "                            callback_on_step_end=inference_callback,\n",
    "                            image=self.last_latent,\n",
    "                            strength=self.cfg.inversion_max_step,\n",
    "                            denoising_start=1.0 - self.cfg.inversion_max_step,\n",
    "                            guidance_scale=guidance_scale).images[0]\n",
    "        return image.resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "    image_size = (1024,1024)\n",
    "    config = RunConfig(num_inference_steps=20,\n",
    "                   num_inversion_steps=20,\n",
    "                   guidance_scale=0.0,\n",
    "                   inversion_max_step=0.6) #4,4,0,0.6 is default settings 0.6 and 0.7 look the same\n",
    "else:\n",
    "    image_size = (512,512)\n",
    "    config = RunConfig(num_inference_steps=4,\n",
    "                   num_inversion_steps=4,\n",
    "                   guidance_scale=0.0,\n",
    "                   inversion_max_step=0.6) #4,4,0,0.6 is default settings 0.6 and 0.7 look the same\n",
    "dtype = torch.float32\n",
    "scheduler_class = MyEulerAncestralDiscreteScheduler\n",
    "if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "    pipe_inversion = SDXLDDIMPipeline.from_pretrained(model, \n",
    "                                                      torch_dtype=dtype,\n",
    "                                                      device_map=\"balanced\",\n",
    "                                                      variant=(\"fp16\" if dtype==torch.float16 else None))\n",
    "    \n",
    "    pipe_inference = HookedStableDiffusionXLImg2ImgPipeline.from_pretrained(model, \n",
    "                                                                        torch_dtype=dtype,\n",
    "                                                                        device_map=\"balanced\",\n",
    "                                                                        variant=(\"fp16\" if dtype==torch.float16 else None)\n",
    "                                                                    )\n",
    "    if dtype == torch.float32:\n",
    "        pipe_inversion.text_encoder_2.to(dtype)\n",
    "        pipe_inference.text_encoder_2.to(dtype)\n",
    "else:\n",
    "    pipe_inversion = SDXLDDIMPipeline.from_pretrained(model, use_safetensors=True, safety_checker=None, cache_dir=os.environ[\"HF_HOME\"]).to(device)\n",
    "    pipe_inference = HookedStableDiffusionXLImg2ImgPipeline.from_pretrained(model, use_safetensors=True, safety_checker=None, cache_dir=os.environ[\"HF_HOME\"]).to(device)\n",
    "\n",
    "#pipe_inference = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sdxl-turbo\", use_safetensors=True, safety_checker= None, cache_dir=os.environ[\"HF_HOME\"]).to(device)\n",
    "pipe_inference.scheduler            = scheduler_class.from_config(pipe_inference.scheduler.config)\n",
    "pipe_inversion.scheduler            = scheduler_class.from_config(pipe_inversion.scheduler.config)\n",
    "pipe_inversion.scheduler_inference  = scheduler_class.from_config(pipe_inference.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = display(display_id='my-display')\n",
    "latents, prompts = next(iter(loader))\n",
    "latents *= pipe_inference.vae.config.scaling_factor\n",
    "editor = ImageEditorDemo(pipe_inversion, pipe_inference, latents[0].unsqueeze(0).cuda(), prompts[0], config, edit_cfg=1.2) \n",
    "print(prompts[0])\n",
    "h.display(editor.edit(prompts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxlsae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
