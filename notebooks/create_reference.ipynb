{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/tmp/wendler/hf_cache\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import argparse\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add necessary paths for custom modules\n",
    "sys.path.append(\"/share/u/wendler/code/my-sdxl-unbox\")\n",
    "\n",
    "from SDLens import HookedStableDiffusionXLPipeline\n",
    "from SAE import SparseAutoencoder\n",
    "from utils import add_feature_on_area_turbo\n",
    "\n",
    "import supervision as sv\n",
    "import pycocotools.mask as mask_util\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounded SAM2 and Grounding DINO imports\n",
    "sys.path.append(\"/share/u/wendler/code/Grounded-SAM-2\")\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from grounding_dino.groundingdino.util.inference import load_model, predict\n",
    "import grounding_dino.groundingdino.datasets.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "k = 160\n",
    "exp = 4\n",
    "n_steps = 4\n",
    "m1 = 1.\n",
    "k_transfer = 80*4\n",
    "use_down = True\n",
    "use_up = True\n",
    "use_up0 = True\n",
    "use_mid = True\n",
    "n_examples_per_edit = 1000\n",
    "prefix = '../results/PIE-Bench-final'\n",
    "path_to_checkpoints = \"/share/u/wendler/code/my-sdxl-unbox/hparam_study/\"\n",
    "dtype = \"float32\"\n",
    "mode = \"sae_1\"\n",
    "keep_spatial_info = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_block = {\n",
    "        \"down.2.1\": \"unet.down_blocks.2.attentions.1\",\n",
    "        \"up.0.1\": \"unet.up_blocks.0.attentions.1\",\n",
    "        \"up.0.0\": \"unet.up_blocks.0.attentions.0\",\n",
    "        \"mid.0\": \"unet.mid_block.attentions.0\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SDLens/src to sys.path at the top of the scrip\n",
    "\n",
    "# --- Utility functions ---\n",
    "def resize_mask(mask, size=(16, 16)):\n",
    "    # consider all 32 by 32 windows in the mask\n",
    "    small = cv2.resize(mask.astype(np.float32), size, interpolation=cv2.INTER_LANCZOS4) > 0\n",
    "    if small.astype(np.float32).sum() == 0:\n",
    "        tmp = mask.reshape(16, 32, 16, 32).astype(np.float32)\n",
    "        tmp = tmp.sum(axis=1)\n",
    "        tmp = tmp.sum(axis=2)\n",
    "        if (tmp >= 32*32).astype(np.float32).sum() == 0:\n",
    "            print(\"trying to fix the mask...\")\n",
    "            # set the maximum gridcell to 1\n",
    "            amax = tmp.argmax()\n",
    "            tmp[np.unravel_index(amax, tmp.shape)] = 1\n",
    "            return tmp.astype(bool)\n",
    "    return small\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple\n",
    "import grounding_dino.groundingdino.datasets.transforms as T\n",
    "\n",
    "def sam_mask(img, prompt, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD):\n",
    "    def load_image(img) -> Tuple[np.array, torch.Tensor]:\n",
    "        transform = T.Compose(\n",
    "            [\n",
    "                T.RandomResize([800], max_size=1333),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        image_source = img.convert(\"RGB\")\n",
    "        image = np.asarray(image_source)\n",
    "        image_transformed, _ = transform(image_source, None)\n",
    "        return image, image_transformed\n",
    "    image_source, image = load_image(img)\n",
    "    sam2_predictor.set_image(image_source)\n",
    "\n",
    "    boxes, confidences, labels = predict(\n",
    "        model=grounding_model,\n",
    "        image=image,\n",
    "        caption=prompt,\n",
    "        box_threshold=BOX_THRESHOLD,\n",
    "        text_threshold=TEXT_THRESHOLD,\n",
    "    )\n",
    "\n",
    "    # process the box prompt for SAM 2\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    input_boxes = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "\n",
    "\n",
    "    # FIXME: figure how does this influence the G-DINO model\n",
    "    # torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "    #if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        #torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        #torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    masks, scores, logits = sam2_predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Post-process the output of the model to get the masks, scores, and logits for visualization\n",
    "    \"\"\"\n",
    "    # convert the shape to (n, H, W)\n",
    "    if masks.ndim == 4:\n",
    "        masks = masks.squeeze(1)\n",
    "\n",
    "\n",
    "    confidences = confidences.numpy().tolist()\n",
    "    class_names = labels\n",
    "\n",
    "    class_ids = np.array(list(range(len(class_names))))\n",
    "\n",
    "    labels = [\n",
    "        f\"{class_name} {confidence:.2f}\"\n",
    "        for class_name, confidence\n",
    "        in zip(class_names, confidences)\n",
    "    ]\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=input_boxes,  # (n, 4)\n",
    "        mask=masks.astype(bool),  # (n, h, w)\n",
    "        class_id=class_ids\n",
    "    )\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated_frame = box_annotator.annotate(scene=img.copy(), detections=detections)\n",
    "\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    return detections, labels, annotated_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "539fa134944f4ecb86429732adb312ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if dtype == \"float16\":\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pipe = HookedStableDiffusionXLPipeline.from_pretrained(\n",
    "    'stabilityai/sdxl-turbo',\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"balanced\",\n",
    "    variant=(\"fp16\" if dtype==torch.float16 else None)\n",
    ")\n",
    "if dtype == torch.float32:\n",
    "    pipe.text_encoder_2.to(dtype=dtype)\n",
    "pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "SAM2_CHECKPOINT = \"/share/u/wendler/code/Grounded-SAM-2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_MODEL_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "GROUNDING_DINO_CONFIG = \"/share/u/wendler/code/Grounded-SAM-2/grounding_dino/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT = \"/share/u/wendler/code/Grounded-SAM-2/gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
    "BOX_THRESHOLD = 0.35\n",
    "TEXT_THRESHOLD = 0.25\n",
    "sam2_model = build_sam2(SAM2_MODEL_CONFIG, SAM2_CHECKPOINT, device=device)\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "grounding_model = load_model(\n",
    "    model_config_path=GROUNDING_DINO_CONFIG,\n",
    "    model_checkpoint_path=GROUNDING_DINO_CHECKPOINT,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "def create_reference_images(prompt1, prompt2, gsam_prompt1, gsam_prompt2, pipe=pipe, \n",
    "         n_steps=1, verbose=False,\n",
    "         sam_predictor=sam2_predictor, grounding_model=grounding_model, \n",
    "         result_name=None):\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    if verbose:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        logger.setLevel(logging.INFO)    \n",
    "    \n",
    "    seed = 42\n",
    "    base_imgs1, cache1 = pipe.run_with_cache(\n",
    "        prompt1,\n",
    "        positions_to_cache=[],\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    base_imgs2, cache2 = pipe.run_with_cache(\n",
    "        prompt2,\n",
    "        positions_to_cache=[],\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    img1 = base_imgs1[0][0]\n",
    "    img2 = base_imgs2[0][0]\n",
    "\n",
    "    # save to path\n",
    "    img1.save(f\"{result_name}_img1.png\")\n",
    "    img2.save(f\"{result_name}_img2.png\")\n",
    "\n",
    "    if gsam_prompt1 == \"#everything\":\n",
    "        mask1 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt1:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, \"foreground\", sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "            mask1 = np.logical_not(mask1)\n",
    "            if verbose:\n",
    "                plt.imshow(mask1)\n",
    "                plt.show()\n",
    "        elif \"~\" in gsam_prompt1:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, gsam_prompt1.replace(\"~\", \"\"), sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = ~np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "        else:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, gsam_prompt1, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if gsam_prompt2 == \"#everything\":\n",
    "        mask2 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt2:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, \"foreground\", sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "            mask2 = np.logical_not(mask2)\n",
    "            if verbose:\n",
    "                plt.imshow(mask2)\n",
    "                plt.show()\n",
    "        elif \"~\" in gsam_prompt2:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, gsam_prompt2.replace(\"~\", \"\"), sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = ~np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "        else:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, gsam_prompt2, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if mask1.sum() == 0 or mask2.sum() == 0:\n",
    "        raise ValueError(\"one of the masks is empty\")\n",
    "\n",
    "    # also save the detections objects and labels 2 and annotated frames using pickle\n",
    "    with open(f\"{result_name}_detections1.pkl\", \"wb\") as f:\n",
    "        pickle.dump(detections1, f)\n",
    "    with open(f\"{result_name}_detections2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(detections2, f)\n",
    "    with open(f\"{result_name}_labels1.pkl\", \"wb\") as f:\n",
    "        pickle.dump(labels1, f)\n",
    "    with open(f\"{result_name}_labels2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(labels2, f)\n",
    "    with open(f\"{result_name}_annotated_frame1.pkl\", \"wb\") as f:\n",
    "        pickle.dump(annotated_frame1, f)\n",
    "    with open(f\"{result_name}_annotated_frame2.pkl\", \"wb\") as f:\n",
    "        pickle.dump(annotated_frame2, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000\n",
      "000000000001\n",
      "000000000002\n",
      "000000000003\n",
      "000000000004\n",
      "000000000005\n",
      "000000000006\n",
      "000000000007\n",
      "000000000008\n",
      "000000000009\n",
      "000000000010\n",
      "000000000011\n",
      "000000000012\n",
      "000000000013\n",
      "000000000014\n",
      "000000000015\n",
      "000000000016\n",
      "000000000017\n",
      "000000000018\n",
      "000000000019\n",
      "000000000020\n",
      "000000000021\n",
      "000000000022\n",
      "000000000023\n",
      "000000000024\n",
      "000000000025\n",
      "000000000026\n",
      "000000000027\n",
      "000000000028\n",
      "000000000029\n",
      "000000000030\n",
      "000000000031\n",
      "000000000032\n",
      "000000000033\n",
      "000000000034\n",
      "000000000035\n",
      "000000000036\n",
      "000000000037\n",
      "000000000038\n",
      "000000000039\n",
      "000000000040\n",
      "000000000041\n",
      "000000000042\n",
      "000000000043\n",
      "000000000044\n",
      "000000000045\n",
      "000000000046\n",
      "000000000047\n",
      "000000000048\n",
      "000000000049\n",
      "000000000050\n",
      "000000000051\n",
      "000000000052\n",
      "000000000053\n",
      "000000000054\n",
      "000000000055\n",
      "000000000056\n",
      "000000000057\n",
      "000000000058\n",
      "000000000059\n",
      "000000000060\n",
      "000000000061\n",
      "000000000062\n",
      "000000000063\n",
      "000000000064\n",
      "000000000065\n",
      "000000000066\n",
      "000000000067\n",
      "000000000068\n",
      "000000000069\n",
      "000000000070\n",
      "000000000071\n",
      "000000000072\n",
      "000000000073\n",
      "000000000074\n",
      "000000000075\n",
      "000000000076\n",
      "000000000077\n",
      "000000000078\n",
      "000000000079\n",
      "000000000080\n",
      "000000000081\n",
      "000000000082\n",
      "000000000083\n",
      "000000000084\n",
      "000000000085\n",
      "000000000086\n",
      "000000000087\n",
      "000000000088\n",
      "000000000089\n",
      "000000000090\n",
      "000000000091\n",
      "000000000092\n",
      "000000000093\n",
      "000000000094\n",
      "000000000095\n",
      "000000000096\n",
      "000000000097\n",
      "000000000098\n",
      "000000000099\n",
      "000000000100\n",
      "000000000101\n",
      "000000000102\n",
      "000000000103\n",
      "000000000104\n",
      "000000000105\n",
      "000000000106\n",
      "000000000107\n",
      "000000000108\n",
      "000000000109\n",
      "000000000110\n",
      "000000000111\n",
      "000000000112\n",
      "000000000113\n",
      "000000000114\n",
      "000000000115\n",
      "000000000116\n",
      "000000000117\n",
      "000000000118\n",
      "000000000119\n",
      "000000000120\n",
      "000000000121\n",
      "000000000122\n",
      "000000000123\n",
      "000000000124\n",
      "000000000125\n",
      "000000000126\n",
      "000000000127\n",
      "000000000128\n",
      "000000000129\n",
      "000000000130\n",
      "000000000131\n",
      "000000000132\n",
      "000000000133\n",
      "000000000134\n",
      "000000000135\n",
      "000000000136\n",
      "000000000137\n",
      "000000000138\n",
      "000000000139\n",
      "111000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "UserWarning: Memory efficient kernel not used because: (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:776.)\n",
      "UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h:551.)\n",
      "UserWarning: Flash attention kernel not used because: (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:778.)\n",
      "UserWarning: Expected query, key and value to all be of dtype: {Half, BFloat16}. Got Query dtype: float, Key dtype: float, and Value dtype: float instead. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h:93.)\n",
      "UserWarning: CuDNN attention kernel not used because: (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:780.)\n",
      "UserWarning: Flash Attention kernel failed due to: No available kernel. Aborting execution.\n",
      "Falling back to all available kernels for scaled_dot_product_attention (which may have a slower speed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to fix the mask...\n",
      "trying to fix the mask...\n",
      "trying to fix the mask...\n",
      "111000000001\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "from collections import defaultdict\n",
    "with open(\"./generated_piebench.json\", \"r\") as f:\n",
    "    pb = json.load(f)\n",
    "\n",
    "\n",
    "expid2name= {\"0\":\"random\", # -> remove\n",
    "\"1\":\"change object\", # default\n",
    "\"2\":\"add object\", # done \n",
    "\"3\":\"delete object\", # done \n",
    "\"4\":\"change content\", # possible -> (this should apply to all edits i guess as long as the two versions of the image are similar to each other)take edit mask in original, boost editing mask and surpress orignal mask\n",
    "\"5\":\"change pose\", # somewhat possible -> switch\n",
    "\"6\":\"change color\", # done\n",
    "\"7\":\"change material\",# done \n",
    "\"8\":\"change background\", # done\n",
    "\"9\":\"change style\" # done }\n",
    "}\n",
    "\n",
    "def remove_brakets(txt):\n",
    "     return txt.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "\n",
    "cnt = defaultdict(int)\n",
    "\n",
    "\n",
    "for d in pb:\n",
    "     try:\n",
    "          if d[\"editing_type_id\"] in [] or cnt[d[\"editing_type_id\"]] > n_examples_per_edit:\n",
    "               continue\n",
    "          key = d[\"id\"]\n",
    "          print(key)\n",
    "          path = os.path.join(prefix, f\"reference/{d['editing_type_id']}\")\n",
    "          original_prompt = remove_brakets(d[\"original_prompt\"])\n",
    "          editing_prompt = remove_brakets(d[\"editing_prompt\"])\n",
    "          os.makedirs(path, exist_ok=True)\n",
    "          if d[\"editing_type_id\"] in ['0']:\n",
    "               continue\n",
    "          else:\n",
    "               create_reference_images(editing_prompt, original_prompt, d[\"edit_target\"], d[\"edit_source\"], result_name=f\"{path}/{key}\", n_steps=n_steps)\n",
    "          cnt[d[\"editing_type_id\"]] += 1\n",
    "     except Exception as e:\n",
    "          print(e)\n",
    "          continue\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxlsae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
