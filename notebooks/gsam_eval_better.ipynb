{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/tmp/wendler/hf_cache\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import argparse\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add necessary paths for custom modules\n",
    "sys.path.append(\"/share/u/wendler/code/my-sdxl-unbox\")\n",
    "\n",
    "from SDLens import HookedStableDiffusionXLPipeline\n",
    "from SAE import SparseAutoencoder\n",
    "from utils import add_feature_on_area_turbo\n",
    "\n",
    "import supervision as sv\n",
    "import pycocotools.mask as mask_util\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounded SAM2 and Grounding DINO imports\n",
    "sys.path.append(\"/share/u/wendler/code/Grounded-SAM-2\")\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from grounding_dino.groundingdino.util.inference import load_model, predict\n",
    "import grounding_dino.groundingdino.datasets.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "k = 160\n",
    "exp = 4\n",
    "n_steps = 4\n",
    "m1 = 1.\n",
    "k_transfer = 80*4\n",
    "use_down = True\n",
    "use_up = True\n",
    "use_up0 = True\n",
    "use_mid = True\n",
    "n_examples_per_edit = 10\n",
    "prefix = '../results/PIE-Bench-final'\n",
    "path_to_checkpoints = \"/share/u/wendler/code/my-sdxl-unbox/hparam_study/\"\n",
    "dtype = \"float32\"\n",
    "mode = \"neurons\"\n",
    "keep_spatial_info = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_block = {\n",
    "        \"down.2.1\": \"unet.down_blocks.2.attentions.1\",\n",
    "        \"up.0.1\": \"unet.up_blocks.0.attentions.1\",\n",
    "        \"up.0.0\": \"unet.up_blocks.0.attentions.0\",\n",
    "        \"mid.0\": \"unet.mid_block.attentions.0\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SDLens/src to sys.path at the top of the scrip\n",
    "\n",
    "# --- Utility functions ---\n",
    "def resize_mask(mask, size=(16, 16)):\n",
    "    # consider all 32 by 32 windows in the mask\n",
    "    small = cv2.resize(mask.astype(np.float32), size, interpolation=cv2.INTER_LANCZOS4) > 0\n",
    "    if small.astype(np.float32).sum() == 0:\n",
    "        tmp = mask.reshape(16, 32, 16, 32).astype(np.float32)\n",
    "        tmp = tmp.sum(axis=1)\n",
    "        tmp = tmp.sum(axis=2)\n",
    "        if (tmp >= 32*32).astype(np.float32).sum() == 0:\n",
    "            print(\"trying to fix the mask...\")\n",
    "            # set the maximum gridcell to 1\n",
    "            amax = tmp.argmax()\n",
    "            tmp[np.unravel_index(amax, tmp.shape)] = 1\n",
    "            return tmp.astype(bool)\n",
    "    return small\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple\n",
    "import grounding_dino.groundingdino.datasets.transforms as T\n",
    "\n",
    "def sam_mask(img, prompt, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD):\n",
    "    def load_image(img) -> Tuple[np.array, torch.Tensor]:\n",
    "        transform = T.Compose(\n",
    "            [\n",
    "                T.RandomResize([800], max_size=1333),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        image_source = img.convert(\"RGB\")\n",
    "        image = np.asarray(image_source)\n",
    "        image_transformed, _ = transform(image_source, None)\n",
    "        return image, image_transformed\n",
    "    image_source, image = load_image(img)\n",
    "    sam2_predictor.set_image(image_source)\n",
    "\n",
    "    boxes, confidences, labels = predict(\n",
    "        model=grounding_model,\n",
    "        image=image,\n",
    "        caption=prompt,\n",
    "        box_threshold=BOX_THRESHOLD,\n",
    "        text_threshold=TEXT_THRESHOLD,\n",
    "    )\n",
    "\n",
    "    # process the box prompt for SAM 2\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    input_boxes = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "\n",
    "\n",
    "    # FIXME: figure how does this influence the G-DINO model\n",
    "    # torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "    #if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        #torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        #torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    masks, scores, logits = sam2_predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Post-process the output of the model to get the masks, scores, and logits for visualization\n",
    "    \"\"\"\n",
    "    # convert the shape to (n, H, W)\n",
    "    if masks.ndim == 4:\n",
    "        masks = masks.squeeze(1)\n",
    "\n",
    "\n",
    "    confidences = confidences.numpy().tolist()\n",
    "    class_names = labels\n",
    "\n",
    "    class_ids = np.array(list(range(len(class_names))))\n",
    "\n",
    "    labels = [\n",
    "        f\"{class_name} {confidence:.2f}\"\n",
    "        for class_name, confidence\n",
    "        in zip(class_names, confidences)\n",
    "    ]\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=input_boxes,  # (n, 4)\n",
    "        mask=masks.astype(bool),  # (n, h, w)\n",
    "        class_id=class_ids\n",
    "    )\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated_frame = box_annotator.annotate(scene=img.copy(), detections=detections)\n",
    "\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    return detections, labels, annotated_frame\n",
    "    \n",
    "\n",
    "def best_features_saeuron(source_feats, target_feats, k=10):\n",
    "    mean_source = source_feats.mean(dim=0).mean(dim=0)\n",
    "    mean_target = target_feats.mean(dim=0).mean(dim=0)\n",
    "    scores = mean_source/mean_source.sum() - mean_target/mean_target.sum()\n",
    "    arg_sorted = np.argsort(scores.cpu().detach().numpy())\n",
    "    return arg_sorted[::-1][:k].copy(), arg_sorted[:k].copy()\n",
    "\n",
    "def best_features_neuron(source_feats, target_feats, k=10):\n",
    "    mean_source = source_feats.mean(dim=0).mean(dim=0)\n",
    "    mean_target = target_feats.mean(dim=0).mean(dim=0)\n",
    "    scores = (mean_source/mean_source.sum() - mean_target/mean_target.sum()).abs()\n",
    "    arg_sorted = np.argsort(scores.cpu().detach().numpy())\n",
    "    return arg_sorted[::-1][:k].copy(), arg_sorted[:k].copy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_features_per_block(cache1, cache2, mask1, mask2, k_transfer=None, blocks_to_intervene=None, saes=None):\n",
    "    print(blocks_to_intervene)\n",
    "    to_source_features_dict = {}\n",
    "    to_target_features_dict = {}\n",
    "    source_feats_dict = {}\n",
    "    target_feats_dict = {}\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    for shortcut in blocks_to_intervene:\n",
    "        block = code_to_block[shortcut]\n",
    "        diff1 = cache1['output'][block][0] - cache1['input'][block][0]\n",
    "        diff2 = cache2['output'][block][0] - cache2['input'][block][0]\n",
    "        source = diff1[:, :, mask1]\n",
    "        target = diff2[:, :, mask2]\n",
    "        sae = saes[shortcut]\n",
    "        source_feats = sae.encode(source.permute(0, 2, 1))\n",
    "        target_feats = sae.encode(target.permute(0, 2, 1))\n",
    "        to_source_features, to_target_features = best_features_saeuron(source_feats, target_feats, k=k_transfer)\n",
    "        to_source_features_dict[shortcut] = to_source_features\n",
    "        to_target_features_dict[shortcut] = to_target_features\n",
    "        source_feats_dict[shortcut] = source_feats.detach().cpu()\n",
    "        target_feats_dict[shortcut] = target_feats.detach().cpu()\n",
    "        source_dict[shortcut] = source.detach().cpu()\n",
    "        target_dict[shortcut] = target.detach().cpu()   \n",
    "    return to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict\n",
    "    \n",
    "@torch.no_grad()\n",
    "def get_features_all_blocks(cache1, cache2, mask1, mask2, k_transfer=None, blocks_to_intervene=None, saes=None,\n",
    "                            normalize=False):\n",
    "    source_feats_all = []\n",
    "    target_feats_all = []\n",
    "    to_source_feats_dict = {}\n",
    "    to_target_feats_dict = {}\n",
    "    source_feats_dict = {}\n",
    "    target_feats_dict = {}\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    block_sizes = {}\n",
    "    for shortcut in blocks_to_intervene:\n",
    "        block = code_to_block[shortcut]\n",
    "        diff1 = cache1['output'][block][0] - cache1['input'][block][0]\n",
    "        diff2 = cache2['output'][block][0] - cache2['input'][block][0]\n",
    "        source = diff1[:, :, mask1]\n",
    "        target = diff2[:, :, mask2]\n",
    "        sae = saes[shortcut]\n",
    "        source_feats = sae.encode(source.permute(0, 2, 1))\n",
    "        target_feats = sae.encode(target.permute(0, 2, 1))\n",
    "        if normalize:\n",
    "            source_feats_all.append(source_feats/source_feats.mean(dim=1, keepdim=True).norm(dim=-1, keepdim=True))\n",
    "            target_feats_all.append(target_feats/target_feats.mean(dim=1, keepdim=True).norm(dim=-1, keepdim=True))\n",
    "        else:\n",
    "            source_feats_all.append(source_feats)\n",
    "            target_feats_all.append(target_feats)\n",
    "        source_dict[shortcut] = source.detach().cpu()\n",
    "        target_dict[shortcut] = target.detach().cpu()\n",
    "        source_feats_dict[shortcut] = source_feats.detach().cpu() \n",
    "        target_feats_dict[shortcut] = target_feats.detach().cpu()\n",
    "        block_sizes[shortcut] = source_feats.shape[-1]\n",
    "    source_feats_all = torch.cat(source_feats_all, dim=-1)\n",
    "    target_feats_all = torch.cat(target_feats_all, dim=-1)\n",
    "    to_source_feats_all, to_target_feats_all = best_features_saeuron(source_feats_all, target_feats_all, k=k_transfer)\n",
    "    start = 0\n",
    "    for idx, shortcut in enumerate(blocks_to_intervene):\n",
    "        to_source_feats = []\n",
    "        to_target_feats = []\n",
    "        for feat in to_source_feats_all:\n",
    "            if feat >= start and feat < start+block_sizes[shortcut]:\n",
    "                to_source_feats.append(feat-start)\n",
    "        for feat in to_target_feats_all:\n",
    "            if feat >= start and feat < start + block_sizes[shortcut]:\n",
    "                to_target_feats.append(feat-start)\n",
    "        to_source_feats_dict[shortcut] = np.asarray(to_source_feats)\n",
    "        to_target_feats_dict[shortcut] = np.asarray(to_target_feats)\n",
    "        start += block_sizes[shortcut]\n",
    "\n",
    "    return to_source_feats_dict, to_target_feats_dict, source_feats_dict, target_feats_dict, source_dict, target_dict\n",
    "\n",
    "def get_neuron_layer_name(block, lidx):\n",
    "    return code_to_block[block] + f'.transformer_blocks.{lidx}.ff.net.0'\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_neurons_all_blocks(cache1, cache2, mask1, mask2, k_transfer=None, blocks_to_intervene=None, saes=None,\n",
    "                            normalize=True):\n",
    "    to_source_neurons_dict = {}\n",
    "    to_target_neurons_dict = {}\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    block_sizes = {}\n",
    "    neurons1_all = []\n",
    "    neurons2_all = []\n",
    "    for shortcut in blocks_to_intervene:\n",
    "        block = code_to_block[shortcut]\n",
    "        for lidx in range(10):\n",
    "            layer = get_neuron_layer_name(shortcut, lidx)\n",
    "            neurons1 = cache1['output'][layer][0]\n",
    "            neurons2 = cache2['output'][layer][0]\n",
    "            neurons1 = neurons1[:, mask1.flatten(), :]\n",
    "            neurons2 = neurons2[:, mask2.flatten(), :]\n",
    "            if normalize:\n",
    "                neurons1_all.append(neurons1/neurons1.norm(dim=1, keepdim=True))\n",
    "                neurons2_all.append(neurons2/neurons2.norm(dim=1, keepdim=True))\n",
    "            else:\n",
    "                neurons1_all.append(neurons1)\n",
    "                neurons2_all.append(neurons2)\n",
    "            source_dict[layer] = neurons1.detach().cpu()\n",
    "            target_dict[layer] = neurons2.detach().cpu()\n",
    "            block_sizes[layer] = neurons1.shape[-1]\n",
    "    neurons1_all = torch.cat(neurons1_all, dim=-1)\n",
    "    neurons2_all = torch.cat(neurons2_all, dim=-1)\n",
    "    to_source_neurons_all, to_target_neurons_all = best_features_saeuron(neurons1_all, neurons2_all, k=k_transfer)\n",
    "    start = 0\n",
    "    for idx, shortcut in enumerate(blocks_to_intervene):\n",
    "        for lidx in range(10):\n",
    "            layer = get_neuron_layer_name(shortcut, lidx)\n",
    "            to_source_neurons = []\n",
    "            to_target_neurons = []\n",
    "            for nidx in to_source_neurons_all:\n",
    "                if nidx >= start and nidx < start+block_sizes[layer]:\n",
    "                    to_source_neurons.append(nidx-start)\n",
    "            for nidx in to_target_neurons_all:\n",
    "                if nidx >= start and nidx < start+block_sizes[layer]:\n",
    "                    to_target_neurons.append(nidx-start)\n",
    "            to_source_neurons_dict[layer] = np.asarray(to_source_neurons)\n",
    "            to_target_neurons_dict[layer] = np.asarray(to_target_neurons)\n",
    "            start += block_sizes[layer]\n",
    "    return to_source_neurons_dict, to_target_neurons_dict, source_dict, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9467d16736482c8193f176cdd43d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if dtype == \"float16\":\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    dtype = torch.float32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pipe = HookedStableDiffusionXLPipeline.from_pretrained(\n",
    "    'stabilityai/sdxl-turbo',\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"balanced\",\n",
    "    variant=(\"fp16\" if dtype==torch.float16 else None)\n",
    ")\n",
    "if dtype == torch.float32:\n",
    "    pipe.text_encoder_2.to(dtype=dtype)\n",
    "pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "SAM2_CHECKPOINT = \"/share/u/wendler/code/Grounded-SAM-2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_MODEL_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "GROUNDING_DINO_CONFIG = \"/share/u/wendler/code/Grounded-SAM-2/grounding_dino/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT = \"/share/u/wendler/code/Grounded-SAM-2/gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
    "BOX_THRESHOLD = 0.35\n",
    "TEXT_THRESHOLD = 0.25\n",
    "sam2_model = build_sam2(SAM2_MODEL_CONFIG, SAM2_CHECKPOINT, device=device)\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "grounding_model = load_model(\n",
    "    model_config_path=GROUNDING_DINO_CONFIG,\n",
    "    model_checkpoint_path=GROUNDING_DINO_CHECKPOINT,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "blocks = list(code_to_block.values())\n",
    "saes = {}\n",
    "for shortcut in code_to_block.keys():\n",
    "    block = code_to_block[shortcut]\n",
    "    sae = SparseAutoencoder.load_from_disk(\n",
    "        os.path.join(path_to_checkpoints, f\"{block}_k{k}_hidden{exp*1280:d}_auxk256_bs4096_lr0.0001\", \"final\")\n",
    "    ).to(device, dtype=dtype)\n",
    "    saes[shortcut] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "# --- Main experiment ---\n",
    "def add_featuremaps_and_eps(sae, to_source_features, to_target_features, m1, fmaps, target_mask, module, input, output, eps_source=None):\n",
    "    diff = output[0] - input[0]\n",
    "    coefs = sae.encode(diff.permute(0, 2, 3, 1))\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    # norm adjustment not needed because the columns have norm 1 already!\n",
    "    norm_source = sae.decoder.weight[:, to_source_features].norm(dim=0).sum(dim=0)\n",
    "    norm_target = sae.decoder.weight[:, to_target_features].norm(dim=0).sum(dim=0)\n",
    "    if norm_target > 0:\n",
    "        normadjustment = (norm_source/norm_target)\n",
    "    else:\n",
    "        normadjustment = 0\n",
    "    #print(normadjustment)\n",
    "    mask[0,target_mask][..., to_target_features] -= normadjustment*m1*coefs[0, target_mask][..., to_target_features]\n",
    "    mask[..., to_source_features] += fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    print(to_add.shape)\n",
    "    print(coefs.shape)\n",
    "    print('diff')\n",
    "    print(diff.shape)\n",
    "    print(diff.permute(0, 2, 3, 1).shape)\n",
    "    eps_target = torch.zeros_like(diff, device=diff.device)\n",
    "    eps_target[:, :, target_mask] -= diff[:, :, target_mask] - (coefs[:, target_mask] @ sae.decoder.weight.T).permute(0, 2, 1)\n",
    "    if eps_source is not None:\n",
    "        return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device) + eps_target + eps_source,)\n",
    "    else:\n",
    "        return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device) + eps_target,)\n",
    "\n",
    "def add_featuremaps(sae, to_source_features, to_target_features, m1, fmaps, target_mask, module, input, output):\n",
    "    diff = output[0] - input[0]\n",
    "    coefs = sae.encode(diff.permute(0, 2, 3, 1))\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    # norm adjustment not needed because the columns have norm 1 already!\n",
    "    norm_source = sae.decoder.weight[:, to_source_features].norm(dim=0).sum(dim=0)\n",
    "    norm_target = sae.decoder.weight[:, to_target_features].norm(dim=0).sum(dim=0)\n",
    "    if norm_target > 0:\n",
    "        normadjustment = (norm_source/norm_target)\n",
    "    else:\n",
    "        normadjustment = 0\n",
    "    #print(normadjustment)\n",
    "    mask[0,target_mask][..., to_target_features] -= normadjustment*m1*coefs[0, target_mask][..., to_target_features]\n",
    "    mask[..., to_source_features] += fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device),)\n",
    "\n",
    "def add_featuremaps_rescaling(sae, to_source_features, fmaps, module, input, output):\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    mask[..., to_source_features] += fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    out = output[0]/output[0].norm(dim=1, keepdim=True)\n",
    "    out *= output[0].norm(dim=1, keepdim=True)  - to_add.permute(0, 3, 1, 2).norm(dim=1, keepdim=True)\n",
    "    out += to_add.permute(0, 3, 1, 2).to(output[0].device)\n",
    "    return (out, )\n",
    "\n",
    "def ablation(sae, feature_idcs, fmaps, module, input, output):\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    mask[..., feature_idcs] -= fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device),)\n",
    "\n",
    "def add_activations(delta, module, input, output):\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        return output + delta\n",
    "    else:  \n",
    "        return (output[0] + delta,)\n",
    "\n",
    "def main(prompt1, prompt2, gsam_prompt1, gsam_prompt2, pipe=pipe, k=10, \n",
    "         blocks_to_intervene=[\"down.2.1\", \"up.0.1\", \"up.0.0\", \"mid.0\"],\n",
    "         n_steps=1, m1=1., k_transfer=10, stat=\"quantile\", mode=\"sae_1\",  \n",
    "         combine_blocks=True, use_source_mask_in_both=False, subtract_target_add_source=False,\n",
    "         maintain_spatial_info=False, verbose=False,\n",
    "         sam_predictor=sam2_predictor, grounding_model=grounding_model, saes=saes, \n",
    "         result_name=None):\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    if verbose:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        logger.setLevel(logging.INFO)    \n",
    "    blocks = [code_to_block[shortcut] for shortcut in blocks_to_intervene]\n",
    "    if mode == \"neurons\":\n",
    "        neuron_blocks = []\n",
    "        for shortcut in blocks_to_intervene:\n",
    "            for idx in range(10):\n",
    "                neuron_blocks.append(get_neuron_layer_name(shortcut, idx))\n",
    "        blocks = neuron_blocks\n",
    "    logger.debug(\"[4/9] Generating images and caching activations...\")\n",
    "    seed = 42\n",
    "    base_imgs1, cache1 = pipe.run_with_cache(\n",
    "        prompt1,\n",
    "        positions_to_cache=blocks,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    base_imgs2, cache2 = pipe.run_with_cache(\n",
    "        prompt2,\n",
    "        positions_to_cache=blocks,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    img1 = base_imgs1[0][0]\n",
    "    img2 = base_imgs2[0][0]\n",
    "\n",
    "    logger.debug(\"[5/9] Running Grounded SAM on generated images...\")\n",
    "    if gsam_prompt1 == \"#everything\":\n",
    "        mask1 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt1:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, \"foreground\", sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "            mask1 = np.logical_not(mask1)\n",
    "            if verbose:\n",
    "                plt.imshow(mask1)\n",
    "                plt.show()\n",
    "        elif \"~\" in gsam_prompt1:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, gsam_prompt1.replace(\"~\", \"\"), sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = ~np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "        else:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, gsam_prompt1, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if gsam_prompt2 == \"#everything\":\n",
    "        mask2 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt2:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, \"foreground\", sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "            mask2 = np.logical_not(mask2)\n",
    "            if verbose:\n",
    "                plt.imshow(mask2)\n",
    "                plt.show()\n",
    "        elif \"~\" in gsam_prompt2:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, gsam_prompt2.replace(\"~\", \"\"), sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = ~np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "        else:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, gsam_prompt2, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if mask1.sum() == 0 or mask2.sum() == 0:\n",
    "        raise ValueError(\"one of the masks is empty\")\n",
    "    if use_source_mask_in_both:\n",
    "        detections2, labels2, annotated_frame2 = detections1, labels1, annotated_frame1\n",
    "        mask2 = mask1\n",
    "        gsam_prompt2 = gsam_prompt1\n",
    "    if verbose:\n",
    "        plt.imshow(mask1)\n",
    "        plt.show()\n",
    "        plt.imshow(mask2)\n",
    "        plt.show()\n",
    "    logger.debug(\"[6/9] Extracting latents and encoding features...\")\n",
    "    interventions = {}\n",
    "    if mode == \"neurons\":\n",
    "        to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict = \\\n",
    "            get_neurons_all_blocks(cache1, cache2, mask1, mask2, k_transfer=k_transfer, blocks_to_intervene=blocks_to_intervene)\n",
    "    elif combine_blocks:\n",
    "        to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict = \\\n",
    "            get_features_all_blocks(cache1, cache2, mask1, mask2, k_transfer=k_transfer, blocks_to_intervene=blocks_to_intervene, saes=saes)\n",
    "    else:\n",
    "        to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict = \\\n",
    "            get_features_per_block(cache1, cache2, mask1, mask2, k_transfer=k_transfer, blocks_to_intervene=blocks_to_intervene, saes=saes)\n",
    "    \n",
    "    if mode != \"neurons\":\n",
    "        for shortcut in blocks_to_intervene:\n",
    "            # 1 x 39 x 5120\n",
    "            # set up interventions \n",
    "            to_source_features = to_source_features_dict[shortcut]\n",
    "            to_target_features = to_target_features_dict[shortcut]\n",
    "            source_feats = source_feats_dict[shortcut]\n",
    "            target_feats = target_feats_dict[shortcut]\n",
    "            source = source_dict[shortcut]\n",
    "            target = target_dict[shortcut]\n",
    "            sae = saes[shortcut]\n",
    "            block = code_to_block[shortcut]\n",
    "            logger.debug(\"[7/9] Selecting best features...\")\n",
    "\n",
    "            # use max\n",
    "            if stat == \"max\":\n",
    "                stat1_val = source_feats.max(dim=0)[0].max(dim=0)[0][to_source_features]\n",
    "                stat2_val = target_feats.max(dim=0)[0].max(dim=0)[0][to_target_features]\n",
    "            elif stat == \"mean\":\n",
    "                mymeans1 = []\n",
    "                for fidx in to_source_features:\n",
    "                    coefs = source_feats[..., fidx]\n",
    "                    mymeans1.append(coefs[coefs > 1e-3].mean())\n",
    "                stat1_val = torch.tensor(mymeans1, device=torch.device(\"cuda\"))\n",
    "                mymeans2 = []\n",
    "                for fidx in to_target_features:\n",
    "                    coefs = target_feats[..., fidx]\n",
    "                    mymeans2.append(coefs[coefs > 1e-3].mean())\n",
    "                stat2_val = torch.tensor(mymeans2, device=torch.device(\"cuda\"))\n",
    "            elif stat == \"quantile\":\n",
    "                logger.debug(f\"source_feats shape: {source_feats.shape}\")\n",
    "                dtype = source_feats.dtype\n",
    "                stat1_val = source_feats.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "                stat1_val = stat1_val.to(dtype)[to_source_features]\n",
    "                stat2_val = target_feats.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "                stat2_val = stat2_val.to(dtype)[to_target_features]\n",
    "            else:\n",
    "                ValueError(f\"stat1 {stat} not recognized. Choose from: max, mean\")\n",
    "            logger.debug(f\"mean_vals (max): {stat1_val}\")\n",
    "            logger.debug(\"[9/9] Running SDXL with feature injection...\")\n",
    "            logger.debug(f\"Decoder weight shape: {sae.decoder.weight.shape}\")\n",
    "            logger.debug(f\"Using mode: {mode}\")\n",
    "            if mode == \"adding\":\n",
    "                # create the batch x feats x y x x tensor to add\n",
    "                delta = torch.zeros((1, 1280, 16, 16), device=device)\n",
    "                if maintain_spatial_info:\n",
    "                    delta[:, :, mask1] += m1*source.mean(dim=0, keepdim=True).to(delta.device) \n",
    "                    delta[:, :, mask1] -= m1*target.mean(dim=1, keepdim=True).to(delta.device)\n",
    "                else:\n",
    "                    delta[:, :, mask2] += m1*source.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).to(delta.device)\n",
    "                    delta[:, :, mask2] -= m1*target.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).to(delta.device)\n",
    "                f = partial(add_activations, delta.to(dtype=source.dtype))\n",
    "                interventions[block] = f\n",
    "            elif mode == \"steering\":\n",
    "                # create the batch x feats x y x x tensor to add\n",
    "                delta = torch.zeros((1, 1280, 16, 16), dtype=source.dtype, device=device)\n",
    "                if subtract_target_add_source:\n",
    "                    if maintain_spatial_info:\n",
    "                        delta[:, :, mask1] += m1*source.mean(dim=0, keepdim=True).to(delta.device)\n",
    "                        delta[:, :, mask2] -= m1*target.mean(dim=0, keepdim=True).to(delta.device)\n",
    "                    else:\n",
    "                        delta[:, :, mask1] += m1*source.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).to(delta.device)\n",
    "                        delta[:, :, mask2] -= m1*target.mean(dim=0, keepdim=True).to(delta.device)\n",
    "                else:\n",
    "                    delta[:, :, mask2] += m1*source.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).to(delta.device)\n",
    "                    delta[:, :, mask2] -= m1*target.mean(dim=0, keepdim=True).to(delta.device)\n",
    "                f = partial(add_activations, delta)\n",
    "                interventions[block] = f\n",
    "            elif mode == \"sae_1\":\n",
    "                if subtract_target_add_source:       \n",
    "                    fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "                    if maintain_spatial_info:\n",
    "                        fmaps[:, mask1] += m1*source_feats.mean(dim=0, keepdim=True)[..., to_source_features].to(fmaps.device)\n",
    "                    else:\n",
    "                        fmaps[:, mask1] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                    f = partial(add_featuremaps, sae, to_source_features, to_target_features, m1, fmaps, mask2)\n",
    "                else:\n",
    "                    fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "                    fmaps[:, mask2] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                    f = partial(add_featuremaps, sae, to_source_features, to_target_features, m1, fmaps, mask2)\n",
    "                interventions[block] = f\n",
    "            elif mode == \"sae_eps\":\n",
    "                raise ValueError(\"deprecated\")\n",
    "                eps_source = torch.zeros((1, 1280, 16, 16), device=device, dtype=source.dtype)\n",
    "                eps_source[:, :, mask1] += source.mean(dim=0, keepdim=True).to(eps_source.device)\n",
    "                eps_source[:, :, mask1] -= (source_feats.to(device=eps_source.device) @ sae.decoder.weight.T).permute(0, 2, 1).mean(dim=0, keepdim=True).to(eps_source.device)    \n",
    "                if subtract_target_add_source:        \n",
    "                    fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "                    if maintain_spatial_info:\n",
    "                        fmaps[:, mask1] += m1*source_feats.mean(dim=0, keepdim=True)[..., to_source_features].to(fmaps.device)\n",
    "                    else:\n",
    "                        fmaps[:, mask1] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                        tmp = eps_source.mean(dim=2, keepdim=True).mean(dim=3, keepdim=False)\n",
    "                        eps_source -= eps_source\n",
    "                        eps_source[:, :, mask1] += tmp\n",
    "                    f = partial(add_featuremaps_and_eps, sae, to_source_features, to_target_features, m1, fmaps, mask2, eps_source=eps_source)\n",
    "                else:\n",
    "                    fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "                    fmaps[:, mask2] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                    tmp = eps_source.mean(dim=2, keepdim=True).mean(dim=3, keepdim=False)\n",
    "                    eps_source -= eps_source\n",
    "                    eps_source[:, :, mask2] += tmp\n",
    "                    f = partial(add_featuremaps_and_eps, sae, to_source_features, to_target_features, m1, fmaps, mask2, eps_source=eps_source)\n",
    "                interventions[block] = f\n",
    "            elif mode == \"sae_2\":\n",
    "                ValueError(\"deprecated\")\n",
    "                fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "                fmaps[:, mask2] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                f = partial(add_featuremaps_rescaling, sae, to_source_features, fmaps)\n",
    "                interventions[block] = f\n",
    "            elif mode == \"sae_ablation\":\n",
    "                ValueError(\"deprecated\")\n",
    "                fmaps = torch.zeros((1, 16, 16, len(to_target_features)), device=device)\n",
    "                fmaps[:, mask2] += (m1*stat2_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                f = partial(ablation, sae, to_target_features, fmaps)\n",
    "                interventions[block] = f\n",
    "            else:\n",
    "                ValueError(f\"Mode {mode} not recognized. Choose from: patch_max, patch_mean, sae_1, sae_2\")\n",
    "    else:\n",
    "        for shortcut in blocks_to_intervene:\n",
    "            for lidx in range(10):\n",
    "                layer = get_neuron_layer_name(shortcut, lidx)\n",
    "                # 1 x 39 x 5120\n",
    "                # set up interventions \n",
    "                to_source_neurons = to_source_features_dict[layer]\n",
    "                to_target_neurons = to_target_features_dict[layer]\n",
    "                source_neurons = source_feats_dict[layer]\n",
    "                target_neurons = target_feats_dict[layer]\n",
    "                if verbose:\n",
    "                    plt.hist(source_neurons.flatten(), bins=\"rice\")\n",
    "                    plt.show()\n",
    "\n",
    "                if stat == \"quantile\":\n",
    "                    dtype = source_neurons.dtype\n",
    "                    stat1_val = source_neurons.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "                    stat1_val = stat1_val.to(dtype)[to_source_neurons]\n",
    "                    stat2_val = target_neurons.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "                    stat2_val = stat2_val.to(dtype)[to_target_neurons] \n",
    "                else:\n",
    "                    raise ValueError(f\"stat {stat} not supported\")\n",
    "\n",
    "                if subtract_target_add_source:        \n",
    "                    fmaps = torch.zeros((1, 16 * 16, 5120), device=device, dtype=source_neurons.dtype)\n",
    "                    if maintain_spatial_info:\n",
    "                        fmaps = torch.zeros((1, 16 * 16, 5120), device=device, dtype=source_neurons.dtype)\n",
    "                        for i, idx in enumerate(to_source_neurons):\n",
    "                            fmaps[:, mask1.flatten(), idx] += m1 * source_neurons[..., idx].mean(dim=0, keepdim=True).to(fmaps.device)\n",
    "                        for i, idx in enumerate(to_target_neurons):\n",
    "                            fmaps[:, mask2.flatten(), idx] -= m1 * target_neurons[..., idx].mean(dim=0, keepdim=True).to(fmaps.device)\n",
    "                    else:\n",
    "                        for i, idx in enumerate(to_source_neurons):\n",
    "                            fmaps[:, mask1.flatten(), idx] += (m1*stat1_val[i]).to(fmaps.device)\n",
    "                            \n",
    "                        for i, idx in enumerate(to_target_neurons):\n",
    "                            fmaps[:, mask2.flatten(), idx] -= m1 * target_neurons[..., idx].mean(dim=0, keepdim=True).to(fmaps.device)\n",
    "                        else:\n",
    "                            print(\"Warning: Invalid indices detected. Skipping intervention.\")\n",
    "                    f = partial(add_activations, fmaps)\n",
    "                    interventions[layer] = f\n",
    "                else:\n",
    "                    fmaps = torch.zeros((1, 16 * 16, 5120), device=device, dtype=source_neurons.dtype)\n",
    "                    for i, idx in enumerate(to_source_neurons):\n",
    "                        fmaps[:, mask2.flatten(), idx] += (m1*stat1_val[i]).to(fmaps.device)\n",
    "                    for i, idx in enumerate(to_target_neurons):\n",
    "                        fmaps[:, mask2.flatten(), idx] -= m1 * target_neurons[..., idx].mean(dim=0, keepdim=True).to(fmaps.device)\n",
    "                    f = partial(add_activations, fmaps)\n",
    "                    interventions[layer] = f\n",
    "                \n",
    "\n",
    "    result = pipe.run_with_hooks(\n",
    "        prompt2,\n",
    "        position_hook_dict=interventions,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed)\n",
    "    ).images[0]\n",
    "\n",
    "    # make a result figure that shows the images with masks and the intervened image\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Image 1 with mask from prompt 1\n",
    "    if \"~\" not in gsam_prompt1 and \"background\" not in gsam_prompt1 and gsam_prompt1 != \"#everything\":\n",
    "        axs[0].imshow(annotated_frame1)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt1 or \"~\" in gsam_prompt1:\n",
    "            # upsample mask by 32\n",
    "            mask1 = cv2.resize(mask1.astype(np.float32), (16*32, 16*32), interpolation=cv2.INTER_NEAREST)\n",
    "            # Plot the base image first\n",
    "            axs[0].imshow(img1)\n",
    "            # Then overlay the mask with higher alpha for visibility\n",
    "            axs[0].imshow(mask1, alpha=0.5)\n",
    "        else:\n",
    "            axs[0].imshow(img1)\n",
    "    axs[0].set_title(f\"{prompt1}\")\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    # Image 2 with mask from prompt 2\n",
    "    if \"~\" not in gsam_prompt2 and \"background\" not in gsam_prompt2 and gsam_prompt2 != \"#everything\":\n",
    "        axs[1].imshow(annotated_frame2)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt2 or \"~\" in gsam_prompt2:\n",
    "            # upsample mask by 32\n",
    "            mask2 = cv2.resize(mask2.astype(np.float32), (16*32, 16*32), interpolation=cv2.INTER_NEAREST)\n",
    "            # Plot the base image first\n",
    "            axs[1].imshow(img2)\n",
    "            # Then overlay the mask with higher alpha for visibility\n",
    "            axs[1].imshow(mask2, alpha=0.5)\n",
    "        else:\n",
    "            axs[1].imshow(img2)\n",
    "    axs[1].set_title(f\"{prompt2}\")\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    # Intervened result image\n",
    "    axs[2].imshow(result)\n",
    "    axs[2].axis('off')\n",
    "    # tight\n",
    "    #plt.tight_layout()\n",
    "    if result_name is not None:\n",
    "        plt.savefig(result_name + \"_summary.png\")\n",
    "        plt.close()\n",
    "        # save the images\n",
    "        with open(result_name + \"_feats_and_stats.json\", \"w\") as f:\n",
    "            json.dump({\"to_source_features\": {k:v.tolist() for k,v in to_source_features_dict.items()}, \n",
    "                       \"to_target_features\": {k:v.tolist() for k,v in to_target_features_dict.items()},\n",
    "                       \"m1\": m1,\n",
    "                       \"k_transfer\": k_transfer,\n",
    "                       \"stat\": stat,\n",
    "                       \"mode\": mode,\n",
    "                       \"combine_blocks\": combine_blocks,\n",
    "                       \"blocks_to_intervene\": blocks_to_intervene,\n",
    "                       }, f)\n",
    "        img1.save(result_name + f\"_{gsam_prompt2}_img1.png\")\n",
    "        img2.save(result_name + f\"_{gsam_prompt1}_img2.png\")\n",
    "        result.save(result_name + \".png\")\n",
    "    else:\n",
    "        print(to_source_features_dict)\n",
    "        print(to_target_features_dict)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000000000000\n",
      "000000000001\n",
      "000000000002\n",
      "000000000003\n",
      "000000000004\n",
      "000000000005\n",
      "000000000006\n",
      "000000000007\n",
      "000000000008\n",
      "000000000009\n",
      "000000000010\n",
      "000000000011\n",
      "000000000012\n",
      "000000000013\n",
      "000000000014\n",
      "000000000015\n",
      "000000000016\n",
      "000000000017\n",
      "000000000018\n",
      "000000000019\n",
      "000000000020\n",
      "000000000021\n",
      "000000000022\n",
      "000000000023\n",
      "000000000024\n",
      "000000000025\n",
      "000000000026\n",
      "000000000027\n",
      "000000000028\n",
      "000000000029\n",
      "000000000030\n",
      "000000000031\n",
      "000000000032\n",
      "000000000033\n",
      "000000000034\n",
      "000000000035\n",
      "000000000036\n",
      "000000000037\n",
      "000000000038\n",
      "000000000039\n",
      "000000000040\n",
      "000000000041\n",
      "000000000042\n",
      "000000000043\n",
      "000000000044\n",
      "000000000045\n",
      "000000000046\n",
      "000000000047\n",
      "000000000048\n",
      "000000000049\n",
      "000000000050\n",
      "000000000051\n",
      "000000000052\n",
      "000000000053\n",
      "000000000054\n",
      "000000000055\n",
      "000000000056\n",
      "000000000057\n",
      "000000000058\n",
      "000000000059\n",
      "000000000060\n",
      "000000000061\n",
      "000000000062\n",
      "000000000063\n",
      "000000000064\n",
      "000000000065\n",
      "000000000066\n",
      "000000000067\n",
      "000000000068\n",
      "000000000069\n",
      "000000000070\n",
      "000000000071\n",
      "000000000072\n",
      "000000000073\n",
      "000000000074\n",
      "000000000075\n",
      "000000000076\n",
      "000000000077\n",
      "000000000078\n",
      "000000000079\n",
      "000000000080\n",
      "000000000081\n",
      "000000000082\n",
      "000000000083\n",
      "000000000084\n",
      "000000000085\n",
      "000000000086\n",
      "000000000087\n",
      "000000000088\n",
      "000000000089\n",
      "000000000090\n",
      "000000000091\n",
      "000000000092\n",
      "000000000093\n",
      "000000000094\n",
      "000000000095\n",
      "000000000096\n",
      "000000000097\n",
      "000000000098\n",
      "000000000099\n",
      "000000000100\n",
      "000000000101\n",
      "000000000102\n",
      "000000000103\n",
      "000000000104\n",
      "000000000105\n",
      "000000000106\n",
      "000000000107\n",
      "000000000108\n",
      "000000000109\n",
      "000000000110\n",
      "000000000111\n",
      "000000000112\n",
      "000000000113\n",
      "000000000114\n",
      "000000000115\n",
      "000000000116\n",
      "000000000117\n",
      "000000000118\n",
      "000000000119\n",
      "000000000120\n",
      "000000000121\n",
      "000000000122\n",
      "000000000123\n",
      "000000000124\n",
      "000000000125\n",
      "000000000126\n",
      "000000000127\n",
      "000000000128\n",
      "000000000129\n",
      "000000000130\n",
      "000000000131\n",
      "000000000132\n",
      "000000000133\n",
      "000000000134\n",
      "000000000135\n",
      "000000000136\n",
      "000000000137\n",
      "000000000138\n",
      "000000000139\n",
      "111000000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "UserWarning: Memory efficient kernel not used because: (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:776.)\n",
      "UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h:551.)\n",
      "UserWarning: Flash attention kernel not used because: (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:778.)\n",
      "UserWarning: Expected query, key and value to all be of dtype: {Half, BFloat16}. Got Query dtype: float, Key dtype: float, and Value dtype: float instead. (Triggered internally at /pytorch/aten/src/ATen/native/transformers/sdp_utils_cpp.h:93.)\n",
      "UserWarning: CuDNN attention kernel not used because: (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:780.)\n",
      "UserWarning: Flash Attention kernel failed due to: No available kernel. Aborting execution.\n",
      "Falling back to all available kernels for scaled_dot_product_attention (which may have a slower speed).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to fix the mask...\n",
      "trying to fix the mask...\n",
      "trying to fix the mask...\n",
      "111000000001\n",
      "111000000002\n",
      "111000000003\n",
      "111000000004\n",
      "trying to fix the mask...\n",
      "trying to fix the mask...\n",
      "111000000005\n",
      "111000000006\n",
      "111000000007\n",
      "111000000008\n",
      "111000000009\n",
      "112000000000\n",
      "211000000000\n",
      "211000000001\n",
      "211000000002\n",
      "211000000003\n",
      "211000000004\n",
      "211000000005\n",
      "211000000006\n",
      "trying to fix the mask...\n",
      "211000000007\n",
      "211000000008\n",
      "211000000009\n",
      "212000000000\n",
      "311000000000\n",
      "311000000001\n",
      "311000000002\n",
      "311000000003\n",
      "311000000004\n",
      "311000000005\n",
      "311000000006\n",
      "311000000007\n",
      "311000000008\n",
      "311000000009\n",
      "312000000000\n",
      "411000000000\n",
      "411000000001\n",
      "411000000002\n",
      "411000000003\n",
      "411000000004\n",
      "412000000000\n",
      "412000000001\n",
      "412000000002\n",
      "412000000003\n",
      "412000000004\n",
      "413000000000\n",
      "511000000000\n",
      "511000000001\n",
      "511000000002\n",
      "511000000003\n",
      "511000000004\n",
      "512000000000\n",
      "512000000001\n",
      "512000000002\n",
      "512000000003\n",
      "512000000004\n",
      "513000000000\n",
      "611000000000\n",
      "611000000001\n",
      "trying to fix the mask...\n",
      "trying to fix the mask...\n",
      "trying to fix the mask...\n",
      "trying to fix the mask...\n",
      "trying to fix the mask...\n",
      "611000000002\n",
      "611000000003\n",
      "611000000004\n",
      "\n",
      "612000000000\n",
      "612000000001\n",
      "612000000002\n",
      "612000000003\n",
      "612000000004\n",
      "\n",
      "613000000000\n",
      "trying to fix the mask...\n",
      "613000000001\n",
      "613000000002\n",
      "\n",
      "613000000003\n",
      "711000000000\n",
      "711000000001\n",
      "711000000002\n",
      "711000000003\n",
      "711000000004\n",
      "712000000000\n",
      "712000000001\n",
      "712000000002\n",
      "712000000003\n",
      "712000000004\n",
      "713000000000\n",
      "811000000000\n",
      "811000000001\n",
      "811000000002\n",
      "one of the masks is empty\n",
      "811000000003\n",
      "811000000004\n",
      "811000000005\n",
      "\n",
      "811000000006\n",
      "811000000007\n",
      "811000000008\n",
      "811000000009\n",
      "812000000000\n",
      "812000000001\n",
      "812000000002\n",
      "911000000000\n",
      "911000000001\n",
      "911000000002\n",
      "911000000003\n",
      "911000000004\n",
      "911000000005\n",
      "911000000006\n",
      "911000000007\n",
      "911000000008\n",
      "911000000009\n",
      "912000000000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "from collections import defaultdict\n",
    "with open(\"./generated_piebench.json\", \"r\") as f:\n",
    "    pb = json.load(f)\n",
    "\n",
    "pb[0]\n",
    "blocks_to_intervene = []\n",
    "if use_down:\n",
    "     blocks_to_intervene.append(\"down.2.1\")\n",
    "if use_up:\n",
    "     blocks_to_intervene.append(\"up.0.1\")\n",
    "if use_up0:\n",
    "     blocks_to_intervene.append(\"up.0.0\")\n",
    "if use_mid:\n",
    "     blocks_to_intervene.append(\"mid.0\")\n",
    "\n",
    "\n",
    "expid2name= {\"0\":\"random\", # -> remove\n",
    "\"1\":\"change object\", # default\n",
    "\"2\":\"add object\", # done \n",
    "\"3\":\"delete object\", # done \n",
    "\"4\":\"change content\", # possible -> (this should apply to all edits i guess as long as the two versions of the image are similar to each other)take edit mask in original, boost editing mask and surpress orignal mask\n",
    "\"5\":\"change pose\", # somewhat possible -> switch\n",
    "\"6\":\"change color\", # done\n",
    "\"7\":\"change material\",# done \n",
    "\"8\":\"change background\", # done\n",
    "\"9\":\"change style\" # done }\n",
    "}\n",
    "\n",
    "def remove_brakets(txt):\n",
    "     return txt.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "\n",
    "cnt = defaultdict(int)\n",
    "for d in pb:\n",
    "     try:\n",
    "          if d[\"editing_type_id\"] in [] or cnt[d[\"editing_type_id\"]] > n_examples_per_edit:\n",
    "               continue\n",
    "          if d[\"original_prompt\"].replace(\"]\", \"\").replace(\"[\", \"\") == \\\n",
    "               d[\"editing_prompt\"].replace(\"]\", \"\").replace(\"[\", \"\"):\n",
    "               continue\n",
    "          key = d[\"id\"]\n",
    "          print(key)\n",
    "          path = os.path.join(prefix, f\"mode{mode}_spatial{keep_spatial_info}_down{use_down}_up{use_up}_up0{use_up0}_mid{use_mid}_T{n_steps}_ktrans{k_transfer}_str{m1}/{d['editing_type_id']}\")\n",
    "          original_prompt = remove_brakets(d[\"original_prompt\"])\n",
    "          editing_prompt = remove_brakets(d[\"editing_prompt\"])\n",
    "          os.makedirs(path, exist_ok=True)\n",
    "          if d[\"editing_type_id\"] in ['0']:\n",
    "               continue\n",
    "          elif d[\"editing_type_id\"] in ['2']: # add object\n",
    "               main(editing_prompt, original_prompt, d[\"edit_target\"], d[\"edit_source\"], \n",
    "                    use_source_mask_in_both = True, subtract_target_add_source = True,\n",
    "                    maintain_spatial_info=keep_spatial_info,\n",
    "                    blocks_to_intervene=blocks_to_intervene, combine_blocks=True,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"quantile\", mode=mode, \n",
    "                    result_name=f\"{path}/{key}\")\n",
    "          elif d[\"editing_type_id\"] in ['3']: # delete object\n",
    "               main(original_prompt, original_prompt, \"~\"+d[\"edit_source\"], d[\"edit_source\"], \n",
    "                    blocks_to_intervene=blocks_to_intervene, combine_blocks=True, subtract_target_add_source=False,\n",
    "                    maintain_spatial_info=keep_spatial_info,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"quantile\", mode=mode, \n",
    "                    result_name=f\"{path}/{key}\")\n",
    "          elif d[\"editing_type_id\"] in ['4', '5'] + ['1']: # change\n",
    "               main(editing_prompt, original_prompt, d[\"edit_target\"], d[\"edit_source\"], \n",
    "                    blocks_to_intervene=blocks_to_intervene, combine_blocks=True, subtract_target_add_source=True,\n",
    "                    maintain_spatial_info=keep_spatial_info,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"quantile\", mode=mode, \n",
    "                    result_name=f\"{path}/{key}\")\n",
    "          else: # stylistic change\n",
    "               main(editing_prompt, original_prompt, d[\"edit_target\"], d[\"edit_source\"], \n",
    "                    blocks_to_intervene=blocks_to_intervene, combine_blocks=True, maintain_spatial_info=False,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"quantile\", mode=mode, \n",
    "                    result_name=f\"{path}/{key}\")\n",
    "          cnt[d[\"editing_type_id\"]] += 1\n",
    "     except Exception as e:\n",
    "          print(e)\n",
    "          continue\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxlsae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
