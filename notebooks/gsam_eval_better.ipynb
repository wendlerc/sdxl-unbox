{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/tmp/wendler/hf_cache\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import argparse\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add necessary paths for custom modules\n",
    "sys.path.append(\"/share/u/wendler/code/my-sdxl-unbox\")\n",
    "\n",
    "from SDLens import HookedStableDiffusionXLPipeline\n",
    "from SAE import SparseAutoencoder\n",
    "from utils import add_feature_on_area_turbo\n",
    "\n",
    "import supervision as sv\n",
    "import pycocotools.mask as mask_util\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounded SAM2 and Grounding DINO imports\n",
    "sys.path.append(\"/share/u/wendler/code/Grounded-SAM-2\")\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from grounding_dino.groundingdino.util.inference import load_model, predict\n",
    "import grounding_dino.groundingdino.datasets.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "n_steps = 4\n",
    "m1 = 1.\n",
    "k_transfer = 5\n",
    "use_down = True\n",
    "use_up = True\n",
    "use_up0 = True\n",
    "use_mid = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_block = {\n",
    "        \"down.2.1\": \"unet.down_blocks.2.attentions.1\",\n",
    "        \"up.0.1\": \"unet.up_blocks.0.attentions.1\",\n",
    "        \"up.0.0\": \"unet.up_blocks.0.attentions.0\",\n",
    "        \"mid.0\": \"unet.mid_block.attentions.0\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SDLens/src to sys.path at the top of the scrip\n",
    "\n",
    "# --- Utility functions ---\n",
    "def resize_mask(mask, size=(16, 16)):\n",
    "    # consider all 32 by 32 windows in the mask\n",
    "    small = cv2.resize(mask.astype(np.float32), size, interpolation=cv2.INTER_LANCZOS4) > 0\n",
    "    if small.astype(np.float32).sum() == 0:\n",
    "        tmp = mask.reshape(16, 32, 16, 32).astype(np.float32)\n",
    "        tmp = tmp.sum(axis=1)\n",
    "        tmp = tmp.sum(axis=2)\n",
    "        if (tmp >= 32*32).astype(np.float32).sum() == 0:\n",
    "            print(\"trying to fix the mask...\")\n",
    "            # set the maximum gridcell to 1\n",
    "            amax = tmp.argmax()\n",
    "            tmp[np.unravel_index(amax, tmp.shape)] = 1\n",
    "            return tmp.astype(bool)\n",
    "    return small\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple\n",
    "import grounding_dino.groundingdino.datasets.transforms as T\n",
    "\n",
    "def sam_mask(img, prompt, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD):\n",
    "    def load_image(img) -> Tuple[np.array, torch.Tensor]:\n",
    "        transform = T.Compose(\n",
    "            [\n",
    "                T.RandomResize([800], max_size=1333),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        image_source = img.convert(\"RGB\")\n",
    "        image = np.asarray(image_source)\n",
    "        image_transformed, _ = transform(image_source, None)\n",
    "        return image, image_transformed\n",
    "    image_source, image = load_image(img)\n",
    "    sam2_predictor.set_image(image_source)\n",
    "\n",
    "    boxes, confidences, labels = predict(\n",
    "        model=grounding_model,\n",
    "        image=image,\n",
    "        caption=prompt,\n",
    "        box_threshold=BOX_THRESHOLD,\n",
    "        text_threshold=TEXT_THRESHOLD,\n",
    "    )\n",
    "\n",
    "    # process the box prompt for SAM 2\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    input_boxes = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "\n",
    "\n",
    "    # FIXME: figure how does this influence the G-DINO model\n",
    "    # torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "    #if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        #torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        #torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    masks, scores, logits = sam2_predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Post-process the output of the model to get the masks, scores, and logits for visualization\n",
    "    \"\"\"\n",
    "    # convert the shape to (n, H, W)\n",
    "    if masks.ndim == 4:\n",
    "        masks = masks.squeeze(1)\n",
    "\n",
    "\n",
    "    confidences = confidences.numpy().tolist()\n",
    "    class_names = labels\n",
    "\n",
    "    class_ids = np.array(list(range(len(class_names))))\n",
    "\n",
    "    labels = [\n",
    "        f\"{class_name} {confidence:.2f}\"\n",
    "        for class_name, confidence\n",
    "        in zip(class_names, confidences)\n",
    "    ]\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=input_boxes,  # (n, 4)\n",
    "        mask=masks.astype(bool),  # (n, h, w)\n",
    "        class_id=class_ids\n",
    "    )\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated_frame = box_annotator.annotate(scene=img.copy(), detections=detections)\n",
    "\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    return detections, labels, annotated_frame\n",
    "    \n",
    "\n",
    "def best_features_saeuron(source_feats, target_feats, k=10):\n",
    "    mean_source = source_feats.mean(dim=0).mean(dim=0)\n",
    "    mean_target = target_feats.mean(dim=0).mean(dim=0)\n",
    "    scores = mean_source/mean_source.sum() - mean_target/mean_target.sum()\n",
    "    arg_sorted = np.argsort(scores.cpu().detach().numpy())\n",
    "    return arg_sorted[::-1][:k].copy(), arg_sorted[:k].copy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_features_per_block(cache1, cache2, mask1, mask2, k_transfer=None, blocks_to_intervene=None, saes=None):\n",
    "    print(blocks_to_intervene)\n",
    "    to_source_features_dict = {}\n",
    "    to_target_features_dict = {}\n",
    "    source_feats_dict = {}\n",
    "    target_feats_dict = {}\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    for shortcut in blocks_to_intervene:\n",
    "        block = code_to_block[shortcut]\n",
    "        diff1 = cache1['output'][block][0] - cache1['input'][block][0]\n",
    "        diff2 = cache2['output'][block][0] - cache2['input'][block][0]\n",
    "        source = diff1[:, :, mask1]\n",
    "        target = diff2[:, :, mask2]\n",
    "        sae = saes[shortcut]\n",
    "        source_feats = sae.encode(source.permute(0, 2, 1))\n",
    "        target_feats = sae.encode(target.permute(0, 2, 1))\n",
    "        to_source_features, to_target_features = best_features_saeuron(source_feats, target_feats, k=k_transfer)\n",
    "        to_source_features_dict[shortcut] = to_source_features\n",
    "        to_target_features_dict[shortcut] = to_target_features\n",
    "        source_feats_dict[shortcut] = source_feats.detach().cpu()\n",
    "        target_feats_dict[shortcut] = target_feats.detach().cpu()\n",
    "        source_dict[shortcut] = source.detach().cpu()\n",
    "        target_dict[shortcut] = target.detach().cpu()   \n",
    "    return to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict\n",
    "    \n",
    "@torch.no_grad()\n",
    "def get_features_all_blocks(cache1, cache2, mask1, mask2, k_transfer=None, blocks_to_intervene=None, saes=None):\n",
    "    source_feats_all = []\n",
    "    target_feats_all = []\n",
    "    to_source_feats_dict = {}\n",
    "    to_target_feats_dict = {}\n",
    "    source_feats_dict = {}\n",
    "    target_feats_dict = {}\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    block_sizes = {}\n",
    "    for shortcut in blocks_to_intervene:\n",
    "        block = code_to_block[shortcut]\n",
    "        diff1 = cache1['output'][block][0] - cache1['input'][block][0]\n",
    "        diff2 = cache2['output'][block][0] - cache2['input'][block][0]\n",
    "        source = diff1[:, :, mask1]\n",
    "        target = diff2[:, :, mask2]\n",
    "        sae = saes[shortcut]\n",
    "        source_feats = sae.encode(source.permute(0, 2, 1))\n",
    "        target_feats = sae.encode(target.permute(0, 2, 1))\n",
    "        source_feats_all.append(source_feats/source_feats.mean(dim=1, keepdim=True).norm(dim=-1, keepdim=True))\n",
    "        target_feats_all.append(target_feats/target_feats.mean(dim=1, keepdim=True).norm(dim=-1, keepdim=True))\n",
    "        source_dict[shortcut] = source.detach().cpu()\n",
    "        target_dict[shortcut] = target.detach().cpu()\n",
    "        source_feats_dict[shortcut] = source_feats.detach().cpu() \n",
    "        target_feats_dict[shortcut] = target_feats.detach().cpu()\n",
    "        block_sizes[shortcut] = source_feats.shape[-1]\n",
    "    source_feats_all = torch.cat(source_feats_all, dim=-1)\n",
    "    target_feats_all = torch.cat(target_feats_all, dim=-1)\n",
    "    to_source_feats_all, to_target_feats_all = best_features_saeuron(source_feats_all, target_feats_all, k=k_transfer)\n",
    "    start = 0\n",
    "    for idx, shortcut in enumerate(blocks_to_intervene):\n",
    "        to_source_feats = []\n",
    "        to_target_feats = []\n",
    "        for feat in to_source_feats_all:\n",
    "            if feat >= start and feat < start+block_sizes[shortcut]:\n",
    "                to_source_feats.append(feat-start)\n",
    "        for feat in to_target_feats_all:\n",
    "            if feat >= start and feat < start + block_sizes[shortcut]:\n",
    "                to_target_feats.append(feat-start)\n",
    "        to_source_feats_dict[shortcut] = np.asarray(to_source_feats)\n",
    "        to_target_feats_dict[shortcut] = np.asarray(to_target_feats)\n",
    "        start += block_sizes[shortcut]\n",
    "\n",
    "    return to_source_feats_dict, to_target_feats_dict, source_feats_dict, target_feats_dict, source_dict, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6396afecb4468d881c3fc7f1f4d3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dtype = torch.float16\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pipe = HookedStableDiffusionXLPipeline.from_pretrained(\n",
    "    'stabilityai/sdxl-turbo',\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"balanced\",\n",
    "    variant=(\"fp16\" if dtype==torch.float16 else None)\n",
    ")\n",
    "pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "SAM2_CHECKPOINT = \"/share/u/wendler/code/Grounded-SAM-2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_MODEL_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "GROUNDING_DINO_CONFIG = \"/share/u/wendler/code/Grounded-SAM-2/grounding_dino/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT = \"/share/u/wendler/code/Grounded-SAM-2/gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
    "BOX_THRESHOLD = 0.35\n",
    "TEXT_THRESHOLD = 0.25\n",
    "sam2_model = build_sam2(SAM2_MODEL_CONFIG, SAM2_CHECKPOINT, device=device)\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "grounding_model = load_model(\n",
    "    model_config_path=GROUNDING_DINO_CONFIG,\n",
    "    model_checkpoint_path=GROUNDING_DINO_CHECKPOINT,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "path_to_checkpoints = '/share/u/wendler/code/my-sdxl-unbox/checkpoints/'\n",
    "blocks = list(code_to_block.values())\n",
    "saes = {}\n",
    "k = 10\n",
    "exp = 4\n",
    "for shortcut in code_to_block.keys():\n",
    "    block = code_to_block[shortcut]\n",
    "    sae = SparseAutoencoder.load_from_disk(\n",
    "        os.path.join(path_to_checkpoints, f\"{block}_k{k}_hidden{exp*1280:d}_auxk256_bs4096_lr0.0001\", \"final\")\n",
    "    ).to(device, dtype=dtype)\n",
    "    saes[shortcut] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "# --- Main experiment ---\n",
    "def add_featuremaps(sae, to_source_features, to_target_features, m1, fmaps, target_mask, module, input, output):\n",
    "        diff = output[0] - input[0]\n",
    "        coefs = sae.encode(diff.permute(0, 2, 3, 1))\n",
    "        mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "        mask[0,target_mask][..., to_target_features] -= m1*coefs[0, target_mask][..., to_target_features]\n",
    "        mask[..., to_source_features] += fmaps.to(mask.device)\n",
    "        to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "        return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device),)\n",
    "\n",
    "def add_featuremaps_rescaling(sae, to_source_features, fmaps, module, input, output):\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    mask[..., to_source_features] += fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    out = output[0]/output[0].norm(dim=1, keepdim=True)\n",
    "    out *= output[0].norm(dim=1, keepdim=True)  - to_add.permute(0, 3, 1, 2).norm(dim=1, keepdim=True)\n",
    "    out += to_add.permute(0, 3, 1, 2).to(output[0].device)\n",
    "    return (out, )\n",
    "\n",
    "def ablation(sae, feature_idcs, fmaps, module, input, output):\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    mask[..., feature_idcs] -= fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device),)\n",
    "\n",
    "def activation_patching(mean, target_mask, module, input, output):\n",
    "    diff = output[0] - input[0]\n",
    "    diff[0, :, target_mask] += mean[:, None]\n",
    "    return (diff + input[0],)\n",
    "\n",
    "\n",
    "def main(prompt1, prompt2, gsam_prompt1, gsam_prompt2, pipe=pipe, k=10, \n",
    "         blocks_to_intervene=[\"down.2.1\", \"up.0.1\", \"up.0.0\", \"mid.0\"],\n",
    "         n_steps=1, m1=1., k_transfer=10, stat=\"quantile\", mode=\"sae_1\", \n",
    "         combine_blocks=True, verbose=False,\n",
    "         sam_predictor=sam2_predictor, grounding_model=grounding_model, saes=saes, \n",
    "         result_name=None):\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    if verbose:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        logger.setLevel(logging.INFO)    \n",
    "\n",
    "    logger.debug(\"[4/9] Generating images and caching activations...\")\n",
    "    seed = 42\n",
    "    base_imgs1, cache1 = pipe.run_with_cache(\n",
    "        prompt1,\n",
    "        positions_to_cache=blocks,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    base_imgs2, cache2 = pipe.run_with_cache(\n",
    "        prompt2,\n",
    "        positions_to_cache=blocks,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    img1 = base_imgs1[0][0]\n",
    "    img2 = base_imgs2[0][0]\n",
    "\n",
    "    logger.debug(\"[5/9] Running Grounded SAM on generated images...\")\n",
    "    if gsam_prompt1 == \"#everything\":\n",
    "        mask1 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt1:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, \"foreground\", sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "            mask1 = np.logical_not(mask1)\n",
    "            if verbose:\n",
    "                plt.imshow(mask1)\n",
    "                plt.show()\n",
    "        else:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, gsam_prompt1, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if gsam_prompt2 == \"#everything\":\n",
    "        mask2 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt2:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, \"foreground\", sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "            mask2 = np.logical_not(mask2)\n",
    "            if verbose:\n",
    "                plt.imshow(mask2)\n",
    "                plt.show()\n",
    "        else:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, gsam_prompt2, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if mask1.sum() == 0:\n",
    "        mask1 = np.ones((16, 16), dtype=bool)\n",
    "    if mask2.sum() == 0:\n",
    "        mask2 = np.ones((16, 16), dtype=bool)\n",
    "    if verbose:\n",
    "        plt.imshow(mask1)\n",
    "        plt.show()\n",
    "        plt.imshow(mask2)\n",
    "        plt.show()\n",
    "    logger.debug(\"[6/9] Extracting latents and encoding features...\")\n",
    "    interventions = {}\n",
    "    if combine_blocks:\n",
    "        to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict = \\\n",
    "            get_features_all_blocks(cache1, cache2, mask1, mask2, k_transfer=k_transfer, blocks_to_intervene=blocks_to_intervene, saes=saes)\n",
    "    else:\n",
    "        to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict = \\\n",
    "            get_features_per_block(cache1, cache2, mask1, mask2, k_transfer=k_transfer, blocks_to_intervene=blocks_to_intervene, saes=saes)\n",
    "    \n",
    "    \n",
    "    for shortcut in blocks_to_intervene:\n",
    "        # 1 x 39 x 5120\n",
    "        # set up interventions \n",
    "        to_source_features = to_source_features_dict[shortcut]\n",
    "        to_target_features = to_target_features_dict[shortcut]\n",
    "        source_feats = source_feats_dict[shortcut]\n",
    "        target_feats = target_feats_dict[shortcut]\n",
    "        source = source_dict[shortcut]\n",
    "        target = target_dict[shortcut]\n",
    "        sae = saes[shortcut]\n",
    "        block = code_to_block[shortcut]\n",
    "        logger.debug(\"[7/9] Selecting best features...\")\n",
    "\n",
    "        # use max\n",
    "        if stat == \"max\":\n",
    "            stat1_val = source_feats.max(dim=0)[0].max(dim=0)[0][to_source_features]\n",
    "            stat2_val = target_feats.max(dim=0)[0].max(dim=0)[0][to_target_features]\n",
    "        elif stat == \"mean\":\n",
    "            mymeans1 = []\n",
    "            for fidx in to_source_features:\n",
    "                coefs = source_feats[..., fidx]\n",
    "                mymeans1.append(coefs[coefs > 1e-3].mean())\n",
    "            stat1_val = torch.tensor(mymeans1, device=torch.device(\"cuda\"))\n",
    "            mymeans2 = []\n",
    "            for fidx in to_target_features:\n",
    "                coefs = target_feats[..., fidx]\n",
    "                mymeans2.append(coefs[coefs > 1e-3].mean())\n",
    "            stat2_val = torch.tensor(mymeans2, device=torch.device(\"cuda\"))\n",
    "        elif stat == \"quantile\":\n",
    "            logger.debug(f\"source_feats shape: {source_feats.shape}\")\n",
    "            dtype = source_feats.dtype\n",
    "            stat1_val = source_feats.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "            stat1_val = stat1_val.to(dtype)[to_source_features]\n",
    "            stat2_val = target_feats.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "            stat2_val = stat2_val.to(dtype)[to_target_features]\n",
    "        else:\n",
    "            ValueError(f\"stat1 {stat} not recognized. Choose from: max, mean\")\n",
    "        logger.debug(f\"mean_vals (max): {stat1_val}\")\n",
    "        logger.debug(\"[9/9] Running SDXL with feature injection...\")\n",
    "        logger.debug(f\"Decoder weight shape: {sae.decoder.weight.shape}\")\n",
    "        logger.debug(f\"Using mode: {mode}\")\n",
    "        if mode == \"patch_max\":\n",
    "            f = partial(activation_patching, m1*(source[0].max(dim=1)[0] - target[0].max(dim=1)[0]), mask2)\n",
    "            interventions[block] = f\n",
    "        elif mode == \"patch_mean\":\n",
    "            f = partial(activation_patching, m1*(source[0].mean(dim=1) - target[0].mean(dim=1)), mask2)\n",
    "            interventions[block] = f\n",
    "        elif mode == \"sae_1\":\n",
    "            fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "            fmaps[:, mask2] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "            f = partial(add_featuremaps, sae, to_source_features, to_target_features, m1, fmaps, mask2)\n",
    "            interventions[block] = f\n",
    "        elif mode == \"sae_2\":\n",
    "            fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "            fmaps[:, mask2] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "            f = partial(add_featuremaps_rescaling, sae, to_source_features, fmaps)\n",
    "            interventions[block] = f\n",
    "        elif mode == \"sae_ablation\":\n",
    "            fmaps = torch.zeros((1, 16, 16, len(to_target_features)), device=device)\n",
    "            fmaps[:, mask2] += (m1*stat2_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "            f = partial(ablation, sae, to_target_features, fmaps)\n",
    "            interventions[block] = f\n",
    "        else:\n",
    "            ValueError(f\"Mode {mode} not recognized. Choose from: patch_max, patch_mean, sae_1, sae_2\")\n",
    "\n",
    "    result = pipe.run_with_hooks(\n",
    "        prompt2,\n",
    "        position_hook_dict=interventions,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed)\n",
    "    ).images[0]\n",
    "\n",
    "    # make a result figure that shows the images with masks and the intervened image\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Image 1 with mask from prompt 1\n",
    "    if \"background\" not in gsam_prompt1 and gsam_prompt1 != \"#everything\":\n",
    "        axs[0].imshow(annotated_frame1)\n",
    "    else:\n",
    "        axs[0].imshow(img1)\n",
    "    axs[0].set_title(f\"{prompt1}\")\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    # Image 2 with mask from prompt 2\n",
    "    if \"background\" not in gsam_prompt2 and gsam_prompt2 != \"#everything\":\n",
    "        axs[1].imshow(annotated_frame2)\n",
    "    else:\n",
    "        axs[1].imshow(img2)\n",
    "    axs[1].set_title(f\"{prompt2}\")\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    # Intervened result image\n",
    "    axs[2].imshow(result)\n",
    "    axs[2].axis('off')\n",
    "    # tight\n",
    "    #plt.tight_layout()\n",
    "    if result_name is not None:\n",
    "        plt.savefig(result_name + \"_summary.png\")\n",
    "        plt.close()\n",
    "        # save the images\n",
    "        with open(result_name + \"_feats_and_stats.json\", \"w\") as f:\n",
    "            json.dump({\"to_source_features\": {k:v.tolist() for k,v in to_source_features_dict.items()}, \n",
    "                       \"to_target_features\": {k:v.tolist() for k,v in to_target_features_dict.items()},\n",
    "                       \"m1\": m1,\n",
    "                       \"k_transfer\": k_transfer,\n",
    "                       \"stat\": stat,\n",
    "                       \"mode\": mode,\n",
    "                       \"combine_blocks\": combine_blocks,\n",
    "                       \"blocks_to_intervene\": blocks_to_intervene,\n",
    "                       }, f)\n",
    "        img1.save(result_name + f\"_{gsam_prompt2}_img1.png\")\n",
    "        img2.save(result_name + f\"_{gsam_prompt1}_img2.png\")\n",
    "        result.save(result_name + \".png\")\n",
    "    else:\n",
    "        print(to_source_features_dict)\n",
    "        print(to_target_features_dict)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '000000000000',\n",
       " 'editing_type_id': '0',\n",
       " 'original_prompt': 'a slanted mountain bicycle on the road in front of a building',\n",
       " 'editing_prompt': 'a slanted [rusty] mountain bicycle on the road in front of a building',\n",
       " 'editing_instruction': 'Make the frame of the bike rusty',\n",
       " 'edit_source': 'bike frame',\n",
       " 'edit_target': 'bike frame'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "from collections import defaultdict\n",
    "with open(\"./generated_piebench.json\", \"r\") as f:\n",
    "    pb = json.load(f)\n",
    "\n",
    "pb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing blocks jointly...\n",
      "processing blocks jointly...\n",
      "processing blocks jointly...\n",
      "processing blocks jointly...\n",
      "processing blocks jointly...\n",
      "processing blocks jointly...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     19\u001b[39m           main(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, d[\u001b[33m\"\u001b[39m\u001b[33moriginal_prompt\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m#everything\u001b[39m\u001b[33m\"\u001b[39m, d[\u001b[33m\"\u001b[39m\u001b[33medit_source\u001b[39m\u001b[33m\"\u001b[39m], \n\u001b[32m     20\u001b[39m                blocks_to_intervene=blocks_to_intervene, combine_blocks=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     21\u001b[39m                n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\u001b[33m\"\u001b[39m\u001b[33mquantile\u001b[39m\u001b[33m\"\u001b[39m, k=\u001b[32m10\u001b[39m, mode=\u001b[33m\"\u001b[39m\u001b[33msae_ablation\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     22\u001b[39m                result_name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m      \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m           \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mediting_prompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moriginal_prompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43medit_target\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43medit_source\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m               \u001b[49m\u001b[43mblocks_to_intervene\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocks_to_intervene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m               \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_transfer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_transfer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquantile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msae_1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m               \u001b[49m\u001b[43mresult_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     30\u001b[39m      \u001b[38;5;28mprint\u001b[39m(e)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(prompt1, prompt2, gsam_prompt1, gsam_prompt2, pipe, k, blocks_to_intervene, n_steps, m1, k_transfer, stat, mode, combine_blocks, verbose, sam_predictor, grounding_model, saes, result_name)\u001b[39m\n\u001b[32m     48\u001b[39m seed = \u001b[32m42\u001b[39m\n\u001b[32m     49\u001b[39m base_imgs1, cache1 = pipe.run_with_cache(\n\u001b[32m     50\u001b[39m     prompt1,\n\u001b[32m     51\u001b[39m     positions_to_cache=blocks,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m     save_input=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     56\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m base_imgs2, cache2 = \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpositions_to_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m img1 = base_imgs1[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     66\u001b[39m img2 = base_imgs2[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/my-sdxl-unbox/SDLens/hooked_sd_pipeline.py:108\u001b[39m, in \u001b[36mHookedDiffusionAbstractPipeline.run_with_cache\u001b[39m\u001b[34m(self, positions_to_cache, save_input, save_output, first_n, *args, **kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m hooks = [\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mself\u001b[39m._register_cache_hook(position, cache_input, cache_output) \u001b[38;5;28;01mfor\u001b[39;00m position \u001b[38;5;129;01min\u001b[39;00m positions_to_cache\n\u001b[32m    106\u001b[39m ]\n\u001b[32m    107\u001b[39m hooks = [hook \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks \u001b[38;5;28;01mif\u001b[39;00m hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n\u001b[32m    110\u001b[39m     hook.remove()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:1209\u001b[39m, in \u001b[36mStableDiffusionXLPipeline.__call__\u001b[39m\u001b[34m(self, prompt, prompt_2, height, width, num_inference_steps, timesteps, sigmas, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[39m\n\u001b[32m   1207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ip_adapter_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m ip_adapter_image_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1208\u001b[39m     added_cond_kwargs[\u001b[33m\"\u001b[39m\u001b[33mimage_embeds\u001b[39m\u001b[33m\"\u001b[39m] = image_embeds\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m noise_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestep_cond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m    \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1219\u001b[39m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_classifier_free_guidance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_condition.py:1285\u001b[39m, in \u001b[36mUNet2DConditionModel.forward\u001b[39m\u001b[34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[39m\n\u001b[32m   1274\u001b[39m         sample = upsample_block(\n\u001b[32m   1275\u001b[39m             hidden_states=sample,\n\u001b[32m   1276\u001b[39m             temb=emb,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1282\u001b[39m             encoder_attention_mask=encoder_attention_mask,\n\u001b[32m   1283\u001b[39m         )\n\u001b[32m   1284\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m         sample = \u001b[43mupsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m=\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1288\u001b[39m \u001b[43m            \u001b[49m\u001b[43mres_hidden_states_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[43mres_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1289\u001b[39m \u001b[43m            \u001b[49m\u001b[43mupsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupsample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1292\u001b[39m \u001b[38;5;66;03m# 6. post-process\u001b[39;00m\n\u001b[32m   1293\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.conv_norm_out:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_blocks.py:2673\u001b[39m, in \u001b[36mUpBlock2D.forward\u001b[39m\u001b[34m(self, hidden_states, res_hidden_states_tuple, temb, upsample_size, *args, **kwargs)\u001b[39m\n\u001b[32m   2669\u001b[39m             hidden_states = torch.utils.checkpoint.checkpoint(\n\u001b[32m   2670\u001b[39m                 create_custom_forward(resnet), hidden_states, temb\n\u001b[32m   2671\u001b[39m             )\n\u001b[32m   2672\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2673\u001b[39m         hidden_states = \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2676\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsamplers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/diffusers/models/resnet.py:351\u001b[39m, in \u001b[36mResnetBlock2D.forward\u001b[39m\u001b[34m(self, input_tensor, temb, *args, **kwargs)\u001b[39m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m temb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    350\u001b[39m         hidden_states = hidden_states + temb\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm2\u001b[49m(hidden_states)\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.time_embedding_norm == \u001b[33m\"\u001b[39m\u001b[33mscale_shift\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m temb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/sdxlsae/lib/python3.12/site-packages/torch/nn/modules/module.py:1915\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1912\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1913\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1914\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1915\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1916\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1917\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "blocks_to_intervene = []\n",
    "if use_down:\n",
    "     blocks_to_intervene.append(\"down.2.1\")\n",
    "if use_up:\n",
    "     blocks_to_intervene.append(\"up.0.1\")\n",
    "if use_up0:\n",
    "     blocks_to_intervene.append(\"up.0.0\")\n",
    "if use_mid:\n",
    "     blocks_to_intervene.append(\"mid.0\")\n",
    "\n",
    "\n",
    "for d in pb:\n",
    "     key = d[\"id\"]\n",
    "     try:\n",
    "          path = f\"../results/PIE-Bench-iteration2/down{use_down}_up{use_up}_up0{use_up0}_mid{use_mid}_T{n_steps}_ktrans{k_transfer}_str{m1}/{d['editing_type_id']}\"\n",
    "          os.makedirs(path, exist_ok=True)\n",
    "          if d[\"editing_type_id\"] in ['3']:\n",
    "               main(\"\", d[\"original_prompt\"], \"#everything\", d[\"edit_source\"], \n",
    "                    blocks_to_intervene=blocks_to_intervene, combine_blocks=True,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"quantile\", k=10, mode=\"sae_ablation\", \n",
    "                    result_name=f\"{path}/{key}\")\n",
    "               \n",
    "          else:\n",
    "               main(d[\"editing_prompt\"], d[\"original_prompt\"], d[\"edit_target\"], d[\"edit_source\"], \n",
    "                    blocks_to_intervene=blocks_to_intervene, combine_blocks=True,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"quantile\", k=10, mode=\"sae_1\", \n",
    "                    result_name=f\"{path}/{key}\")\n",
    "     except Exception as e:\n",
    "          print(e)\n",
    "          continue\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxlsae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
