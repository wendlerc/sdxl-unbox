{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/tmp/wendler/hf_cache\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import argparse\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add necessary paths for custom modules\n",
    "sys.path.append(\"/share/u/wendler/code/my-sdxl-unbox\")\n",
    "\n",
    "from SDLens import HookedStableDiffusionXLPipeline\n",
    "from SAE import SparseAutoencoder\n",
    "from utils import add_feature_on_area_turbo\n",
    "\n",
    "import supervision as sv\n",
    "import pycocotools.mask as mask_util\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounded SAM2 and Grounding DINO imports\n",
    "sys.path.append(\"/share/u/wendler/code/Grounded-SAM-2\")\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from grounding_dino.groundingdino.util.inference import load_model, predict\n",
    "import grounding_dino.groundingdino.datasets.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "n_steps = 4\n",
    "m1 = 4.\n",
    "k_transfer = 32\n",
    "use_down = True\n",
    "use_up = True\n",
    "use_up0 = True\n",
    "use_mid = True\n",
    "prefix = '../results/PIE-Bench-debug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_block = {\n",
    "        \"down.2.1\": \"unet.down_blocks.2.attentions.1\",\n",
    "        \"up.0.1\": \"unet.up_blocks.0.attentions.1\",\n",
    "        \"up.0.0\": \"unet.up_blocks.0.attentions.0\",\n",
    "        \"mid.0\": \"unet.mid_block.attentions.0\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SDLens/src to sys.path at the top of the scrip\n",
    "\n",
    "# --- Utility functions ---\n",
    "def resize_mask(mask, size=(16, 16)):\n",
    "    # consider all 32 by 32 windows in the mask\n",
    "    small = cv2.resize(mask.astype(np.float32), size, interpolation=cv2.INTER_LANCZOS4) > 0\n",
    "    if small.astype(np.float32).sum() == 0:\n",
    "        tmp = mask.reshape(16, 32, 16, 32).astype(np.float32)\n",
    "        tmp = tmp.sum(axis=1)\n",
    "        tmp = tmp.sum(axis=2)\n",
    "        if (tmp >= 32*32).astype(np.float32).sum() == 0:\n",
    "            print(\"trying to fix the mask...\")\n",
    "            # set the maximum gridcell to 1\n",
    "            amax = tmp.argmax()\n",
    "            tmp[np.unravel_index(amax, tmp.shape)] = 1\n",
    "            return tmp.astype(bool)\n",
    "    return small\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple\n",
    "import grounding_dino.groundingdino.datasets.transforms as T\n",
    "\n",
    "def sam_mask(img, prompt, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD):\n",
    "    def load_image(img) -> Tuple[np.array, torch.Tensor]:\n",
    "        transform = T.Compose(\n",
    "            [\n",
    "                T.RandomResize([800], max_size=1333),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        image_source = img.convert(\"RGB\")\n",
    "        image = np.asarray(image_source)\n",
    "        image_transformed, _ = transform(image_source, None)\n",
    "        return image, image_transformed\n",
    "    image_source, image = load_image(img)\n",
    "    sam2_predictor.set_image(image_source)\n",
    "\n",
    "    boxes, confidences, labels = predict(\n",
    "        model=grounding_model,\n",
    "        image=image,\n",
    "        caption=prompt,\n",
    "        box_threshold=BOX_THRESHOLD,\n",
    "        text_threshold=TEXT_THRESHOLD,\n",
    "    )\n",
    "\n",
    "    # process the box prompt for SAM 2\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    input_boxes = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "\n",
    "\n",
    "    # FIXME: figure how does this influence the G-DINO model\n",
    "    # torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "    #if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        #torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        #torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    masks, scores, logits = sam2_predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Post-process the output of the model to get the masks, scores, and logits for visualization\n",
    "    \"\"\"\n",
    "    # convert the shape to (n, H, W)\n",
    "    if masks.ndim == 4:\n",
    "        masks = masks.squeeze(1)\n",
    "\n",
    "\n",
    "    confidences = confidences.numpy().tolist()\n",
    "    class_names = labels\n",
    "\n",
    "    class_ids = np.array(list(range(len(class_names))))\n",
    "\n",
    "    labels = [\n",
    "        f\"{class_name} {confidence:.2f}\"\n",
    "        for class_name, confidence\n",
    "        in zip(class_names, confidences)\n",
    "    ]\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=input_boxes,  # (n, 4)\n",
    "        mask=masks.astype(bool),  # (n, h, w)\n",
    "        class_id=class_ids\n",
    "    )\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated_frame = box_annotator.annotate(scene=img.copy(), detections=detections)\n",
    "\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    return detections, labels, annotated_frame\n",
    "    \n",
    "\n",
    "def best_features_saeuron(source_feats, target_feats, k=10):\n",
    "    mean_source = source_feats.mean(dim=0).mean(dim=0)\n",
    "    mean_target = target_feats.mean(dim=0).mean(dim=0)\n",
    "    scores = mean_source/mean_source.sum() - mean_target/mean_target.sum()\n",
    "    arg_sorted = np.argsort(scores.cpu().detach().numpy())\n",
    "    return arg_sorted[::-1][:k].copy(), arg_sorted[:k].copy()\n",
    "\n",
    "def best_features_neuron(source_feats, target_feats, k=10):\n",
    "    mean_source = source_feats.mean(dim=0).mean(dim=0)\n",
    "    mean_target = target_feats.mean(dim=0).mean(dim=0)\n",
    "    scores = (mean_source/mean_source.sum() - mean_target/mean_target.sum()).abs()\n",
    "    arg_sorted = np.argsort(scores.cpu().detach().numpy())\n",
    "    return arg_sorted[::-1][:k].copy(), arg_sorted[:k].copy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_features_per_block(cache1, cache2, mask1, mask2, k_transfer=None, blocks_to_intervene=None, saes=None):\n",
    "    print(blocks_to_intervene)\n",
    "    to_source_features_dict = {}\n",
    "    to_target_features_dict = {}\n",
    "    source_feats_dict = {}\n",
    "    target_feats_dict = {}\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    for shortcut in blocks_to_intervene:\n",
    "        block = code_to_block[shortcut]\n",
    "        diff1 = cache1['output'][block][0] - cache1['input'][block][0]\n",
    "        diff2 = cache2['output'][block][0] - cache2['input'][block][0]\n",
    "        source = diff1[:, :, mask1]\n",
    "        target = diff2[:, :, mask2]\n",
    "        sae = saes[shortcut]\n",
    "        source_feats = sae.encode(source.permute(0, 2, 1))\n",
    "        target_feats = sae.encode(target.permute(0, 2, 1))\n",
    "        to_source_features, to_target_features = best_features_saeuron(source_feats, target_feats, k=k_transfer)\n",
    "        to_source_features_dict[shortcut] = to_source_features\n",
    "        to_target_features_dict[shortcut] = to_target_features\n",
    "        source_feats_dict[shortcut] = source_feats.detach().cpu()\n",
    "        target_feats_dict[shortcut] = target_feats.detach().cpu()\n",
    "        source_dict[shortcut] = source.detach().cpu()\n",
    "        target_dict[shortcut] = target.detach().cpu()   \n",
    "    return to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict\n",
    "    \n",
    "@torch.no_grad()\n",
    "def get_features_all_blocks(cache1, cache2, mask1, mask2, k_transfer=None, blocks_to_intervene=None, saes=None,\n",
    "                            normalize=False):\n",
    "    source_feats_all = []\n",
    "    target_feats_all = []\n",
    "    to_source_feats_dict = {}\n",
    "    to_target_feats_dict = {}\n",
    "    source_feats_dict = {}\n",
    "    target_feats_dict = {}\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    block_sizes = {}\n",
    "    for shortcut in blocks_to_intervene:\n",
    "        block = code_to_block[shortcut]\n",
    "        diff1 = cache1['output'][block][0] - cache1['input'][block][0]\n",
    "        diff2 = cache2['output'][block][0] - cache2['input'][block][0]\n",
    "        source = diff1[:, :, mask1]\n",
    "        target = diff2[:, :, mask2]\n",
    "        sae = saes[shortcut]\n",
    "        source_feats = sae.encode(source.permute(0, 2, 1))\n",
    "        target_feats = sae.encode(target.permute(0, 2, 1))\n",
    "        if normalize:\n",
    "            source_feats_all.append(source_feats/source_feats.mean(dim=1, keepdim=True).norm(dim=-1, keepdim=True))\n",
    "            target_feats_all.append(target_feats/target_feats.mean(dim=1, keepdim=True).norm(dim=-1, keepdim=True))\n",
    "        else:\n",
    "            source_feats_all.append(source_feats)\n",
    "            target_feats_all.append(target_feats)\n",
    "        source_dict[shortcut] = source.detach().cpu()\n",
    "        target_dict[shortcut] = target.detach().cpu()\n",
    "        source_feats_dict[shortcut] = source_feats.detach().cpu() \n",
    "        target_feats_dict[shortcut] = target_feats.detach().cpu()\n",
    "        block_sizes[shortcut] = source_feats.shape[-1]\n",
    "    source_feats_all = torch.cat(source_feats_all, dim=-1)\n",
    "    target_feats_all = torch.cat(target_feats_all, dim=-1)\n",
    "    to_source_feats_all, to_target_feats_all = best_features_saeuron(source_feats_all, target_feats_all, k=k_transfer)\n",
    "    start = 0\n",
    "    for idx, shortcut in enumerate(blocks_to_intervene):\n",
    "        to_source_feats = []\n",
    "        to_target_feats = []\n",
    "        for feat in to_source_feats_all:\n",
    "            if feat >= start and feat < start+block_sizes[shortcut]:\n",
    "                to_source_feats.append(feat-start)\n",
    "        for feat in to_target_feats_all:\n",
    "            if feat >= start and feat < start + block_sizes[shortcut]:\n",
    "                to_target_feats.append(feat-start)\n",
    "        to_source_feats_dict[shortcut] = np.asarray(to_source_feats)\n",
    "        to_target_feats_dict[shortcut] = np.asarray(to_target_feats)\n",
    "        start += block_sizes[shortcut]\n",
    "\n",
    "    return to_source_feats_dict, to_target_feats_dict, source_feats_dict, target_feats_dict, source_dict, target_dict\n",
    "\n",
    "def get_neuron_layer_name(block, lidx):\n",
    "    return code_to_block[block] + f'.transformer_blocks.{lidx}.ff.net.0'\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_neurons_all_blocks(cache1, cache2, mask1, mask2, k_transfer=None, blocks_to_intervene=None, saes=None,\n",
    "                            normalize=True):\n",
    "    to_source_neurons_dict = {}\n",
    "    to_target_neurons_dict = {}\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    block_sizes = {}\n",
    "    neurons1_all = []\n",
    "    neurons2_all = []\n",
    "    for shortcut in blocks_to_intervene:\n",
    "        block = code_to_block[shortcut]\n",
    "        for lidx in range(10):\n",
    "            layer = get_neuron_layer_name(shortcut, lidx)\n",
    "            neurons1 = cache1['output'][layer][0]\n",
    "            neurons2 = cache2['output'][layer][0]\n",
    "            neurons1 = neurons1[:, mask1.flatten(), :]\n",
    "            neurons2 = neurons2[:, mask2.flatten(), :]\n",
    "            if normalize:\n",
    "                neurons1_all.append(neurons1/neurons1.norm(dim=1, keepdim=True))\n",
    "                neurons2_all.append(neurons2/neurons2.norm(dim=1, keepdim=True))\n",
    "            else:\n",
    "                neurons1_all.append(neurons1)\n",
    "                neurons2_all.append(neurons2)\n",
    "            source_dict[layer] = neurons1.detach().cpu()\n",
    "            target_dict[layer] = neurons2.detach().cpu()\n",
    "            block_sizes[layer] = neurons1.shape[-1]\n",
    "    neurons1_all = torch.cat(neurons1_all, dim=-1)\n",
    "    neurons2_all = torch.cat(neurons2_all, dim=-1)\n",
    "    to_source_neurons_all, to_target_neurons_all = best_features_saeuron(neurons1_all, neurons2_all, k=k_transfer)\n",
    "    start = 0\n",
    "    for idx, shortcut in enumerate(blocks_to_intervene):\n",
    "        for lidx in range(10):\n",
    "            layer = get_neuron_layer_name(shortcut, lidx)\n",
    "            to_source_neurons = []\n",
    "            to_target_neurons = []\n",
    "            for nidx in to_source_neurons_all:\n",
    "                if nidx >= start and nidx < start+block_sizes[layer]:\n",
    "                    to_source_neurons.append(nidx-start)\n",
    "            for nidx in to_target_neurons_all:\n",
    "                if nidx >= start and nidx < start+block_sizes[layer]:\n",
    "                    to_target_neurons.append(nidx-start)\n",
    "            to_source_neurons_dict[layer] = np.asarray(to_source_neurons)\n",
    "            to_target_neurons_dict[layer] = np.asarray(to_target_neurons)\n",
    "            start += block_sizes[layer]\n",
    "    return to_source_neurons_dict, to_target_neurons_dict, source_dict, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float16\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pipe = HookedStableDiffusionXLPipeline.from_pretrained(\n",
    "    'stabilityai/sdxl-turbo',\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"balanced\",\n",
    "    variant=(\"fp16\" if dtype==torch.float16 else None)\n",
    ")\n",
    "pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM2_CHECKPOINT = \"/share/u/wendler/code/Grounded-SAM-2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_MODEL_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "GROUNDING_DINO_CONFIG = \"/share/u/wendler/code/Grounded-SAM-2/grounding_dino/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT = \"/share/u/wendler/code/Grounded-SAM-2/gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
    "BOX_THRESHOLD = 0.35\n",
    "TEXT_THRESHOLD = 0.25\n",
    "sam2_model = build_sam2(SAM2_MODEL_CONFIG, SAM2_CHECKPOINT, device=device)\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "grounding_model = load_model(\n",
    "    model_config_path=GROUNDING_DINO_CONFIG,\n",
    "    model_checkpoint_path=GROUNDING_DINO_CHECKPOINT,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "path_to_checkpoints = '/share/u/wendler/code/my-sdxl-unbox/checkpoints/'\n",
    "blocks = list(code_to_block.values())\n",
    "saes = {}\n",
    "k = 10\n",
    "exp = 4\n",
    "for shortcut in code_to_block.keys():\n",
    "    block = code_to_block[shortcut]\n",
    "    sae = SparseAutoencoder.load_from_disk(\n",
    "        os.path.join(path_to_checkpoints, f\"{block}_k{k}_hidden{exp*1280:d}_auxk256_bs4096_lr0.0001\", \"final\")\n",
    "    ).to(device, dtype=dtype)\n",
    "    saes[shortcut] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import json\n",
    "# --- Main experiment ---\n",
    "def add_featuremaps_and_eps(sae, to_source_features, to_target_features, m1, fmaps, target_mask, module, input, output, eps_source=None):\n",
    "    diff = output[0] - input[0]\n",
    "    coefs = sae.encode(diff.permute(0, 2, 3, 1))\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    # norm adjustment not needed because the columns have norm 1 already!\n",
    "    norm_source = sae.decoder.weight[:, to_source_features].norm(dim=0).sum(dim=0)\n",
    "    norm_target = sae.decoder.weight[:, to_target_features].norm(dim=0).sum(dim=0)\n",
    "    if norm_target > 0:\n",
    "        normadjustment = (norm_source/norm_target)\n",
    "    else:\n",
    "        normadjustment = 0\n",
    "    #print(normadjustment)\n",
    "    mask[0,target_mask][..., to_target_features] -= normadjustment*m1*coefs[0, target_mask][..., to_target_features]\n",
    "    mask[..., to_source_features] += fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    print(to_add.shape)\n",
    "    print(coefs.shape)\n",
    "    print('diff')\n",
    "    print(diff.shape)\n",
    "    print(diff.permute(0, 2, 3, 1).shape)\n",
    "    eps_target = torch.zeros_like(diff, device=diff.device)\n",
    "    eps_target[:, :, target_mask] -= diff[:, :, target_mask] - (coefs[:, target_mask] @ sae.decoder.weight.T).permute(0, 2, 1)\n",
    "    if eps_source is not None:\n",
    "        return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device) + eps_target + eps_source,)\n",
    "    else:\n",
    "        return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device) + eps_target,)\n",
    "\n",
    "def add_featuremaps(sae, to_source_features, to_target_features, m1, fmaps, target_mask, module, input, output):\n",
    "    diff = output[0] - input[0]\n",
    "    coefs = sae.encode(diff.permute(0, 2, 3, 1))\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    # norm adjustment not needed because the columns have norm 1 already!\n",
    "    norm_source = sae.decoder.weight[:, to_source_features].norm(dim=0).sum(dim=0)\n",
    "    norm_target = sae.decoder.weight[:, to_target_features].norm(dim=0).sum(dim=0)\n",
    "    if norm_target > 0:\n",
    "        normadjustment = (norm_source/norm_target)\n",
    "    else:\n",
    "        normadjustment = 0\n",
    "    #print(normadjustment)\n",
    "    mask[0,target_mask][..., to_target_features] -= normadjustment*m1*coefs[0, target_mask][..., to_target_features]\n",
    "    mask[..., to_source_features] += fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device),)\n",
    "\n",
    "def add_featuremaps_rescaling(sae, to_source_features, fmaps, module, input, output):\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    mask[..., to_source_features] += fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    out = output[0]/output[0].norm(dim=1, keepdim=True)\n",
    "    out *= output[0].norm(dim=1, keepdim=True)  - to_add.permute(0, 3, 1, 2).norm(dim=1, keepdim=True)\n",
    "    out += to_add.permute(0, 3, 1, 2).to(output[0].device)\n",
    "    return (out, )\n",
    "\n",
    "def ablation(sae, feature_idcs, fmaps, module, input, output):\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    mask[..., feature_idcs] -= fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device),)\n",
    "\n",
    "def add_activations(delta, module, input, output):\n",
    "    return (output[0] + delta,)\n",
    "\n",
    "\n",
    "def main(prompt1, prompt2, gsam_prompt1, gsam_prompt2, pipe=pipe, k=10, \n",
    "         blocks_to_intervene=[\"down.2.1\", \"up.0.1\", \"up.0.0\", \"mid.0\"],\n",
    "         n_steps=1, m1=1., k_transfer=10, stat=\"quantile\", mode=\"sae_1\",  \n",
    "         combine_blocks=True, use_source_mask_in_both=False, subtract_target_add_source=False,\n",
    "         maintain_spatial_info=False, verbose=False,\n",
    "         sam_predictor=sam2_predictor, grounding_model=grounding_model, saes=saes, \n",
    "         result_name=None):\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    if verbose:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        logger.setLevel(logging.INFO)    \n",
    "    blocks = [code_to_block[shortcut] for shortcut in blocks_to_intervene]\n",
    "    if mode == \"neurons\":\n",
    "        neuron_blocks = []\n",
    "        for shortcut in blocks_to_intervene:\n",
    "            for idx in range(10):\n",
    "                neuron_blocks.append(get_neuron_layer_name(shortcut, idx))\n",
    "        blocks = neuron_blocks\n",
    "    logger.debug(\"[4/9] Generating images and caching activations...\")\n",
    "    seed = 42\n",
    "    base_imgs1, cache1 = pipe.run_with_cache(\n",
    "        prompt1,\n",
    "        positions_to_cache=blocks,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    base_imgs2, cache2 = pipe.run_with_cache(\n",
    "        prompt2,\n",
    "        positions_to_cache=blocks,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    img1 = base_imgs1[0][0]\n",
    "    img2 = base_imgs2[0][0]\n",
    "\n",
    "    logger.debug(\"[5/9] Running Grounded SAM on generated images...\")\n",
    "    if gsam_prompt1 == \"#everything\":\n",
    "        mask1 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt1:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, \"foreground\", sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "            mask1 = np.logical_not(mask1)\n",
    "            if verbose:\n",
    "                plt.imshow(mask1)\n",
    "                plt.show()\n",
    "        elif \"~\" in gsam_prompt1:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, gsam_prompt1.replace(\"~\", \"\"), sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = ~np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "        else:\n",
    "            detections1, labels1, annotated_frame1 = sam_mask(img1, gsam_prompt1, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "            mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if gsam_prompt2 == \"#everything\":\n",
    "        mask2 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt2:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, \"foreground\", sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "            mask2 = np.logical_not(mask2)\n",
    "            if verbose:\n",
    "                plt.imshow(mask2)\n",
    "                plt.show()\n",
    "        elif \"~\" in gsam_prompt2:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, gsam_prompt2.replace(\"~\", \"\"), sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = ~np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "        else:\n",
    "            detections2, labels2, annotated_frame2 = sam_mask(img2, gsam_prompt2, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "            masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "            mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if mask1.sum() == 0 or mask2.sum() == 0:\n",
    "        raise ValueError(\"one of the masks is empty\")\n",
    "    if use_source_mask_in_both:\n",
    "        detections2, labels2, annotated_frame2 = detections1, labels1, annotated_frame1\n",
    "        mask2 = mask1\n",
    "        gsam_prompt2 = gsam_prompt1\n",
    "    if verbose:\n",
    "        plt.imshow(mask1)\n",
    "        plt.show()\n",
    "        plt.imshow(mask2)\n",
    "        plt.show()\n",
    "    logger.debug(\"[6/9] Extracting latents and encoding features...\")\n",
    "    interventions = {}\n",
    "    if mode == \"neurons\":\n",
    "        to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict = \\\n",
    "            get_neurons_all_blocks(cache1, cache2, mask1, mask2, k_transfer=k_transfer, blocks_to_intervene=blocks_to_intervene)\n",
    "    elif combine_blocks:\n",
    "        to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict = \\\n",
    "            get_features_all_blocks(cache1, cache2, mask1, mask2, k_transfer=k_transfer, blocks_to_intervene=blocks_to_intervene, saes=saes)\n",
    "    else:\n",
    "        to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict = \\\n",
    "            get_features_per_block(cache1, cache2, mask1, mask2, k_transfer=k_transfer, blocks_to_intervene=blocks_to_intervene, saes=saes)\n",
    "    \n",
    "    if mode != \"neurons\":\n",
    "        for shortcut in blocks_to_intervene:\n",
    "            # 1 x 39 x 5120\n",
    "            # set up interventions \n",
    "            to_source_features = to_source_features_dict[shortcut]\n",
    "            to_target_features = to_target_features_dict[shortcut]\n",
    "            source_feats = source_feats_dict[shortcut]\n",
    "            target_feats = target_feats_dict[shortcut]\n",
    "            source = source_dict[shortcut]\n",
    "            target = target_dict[shortcut]\n",
    "            sae = saes[shortcut]\n",
    "            block = code_to_block[shortcut]\n",
    "            logger.debug(\"[7/9] Selecting best features...\")\n",
    "\n",
    "            # use max\n",
    "            if stat == \"max\":\n",
    "                stat1_val = source_feats.max(dim=0)[0].max(dim=0)[0][to_source_features]\n",
    "                stat2_val = target_feats.max(dim=0)[0].max(dim=0)[0][to_target_features]\n",
    "            elif stat == \"mean\":\n",
    "                mymeans1 = []\n",
    "                for fidx in to_source_features:\n",
    "                    coefs = source_feats[..., fidx]\n",
    "                    mymeans1.append(coefs[coefs > 1e-3].mean())\n",
    "                stat1_val = torch.tensor(mymeans1, device=torch.device(\"cuda\"))\n",
    "                mymeans2 = []\n",
    "                for fidx in to_target_features:\n",
    "                    coefs = target_feats[..., fidx]\n",
    "                    mymeans2.append(coefs[coefs > 1e-3].mean())\n",
    "                stat2_val = torch.tensor(mymeans2, device=torch.device(\"cuda\"))\n",
    "            elif stat == \"quantile\":\n",
    "                logger.debug(f\"source_feats shape: {source_feats.shape}\")\n",
    "                dtype = source_feats.dtype\n",
    "                stat1_val = source_feats.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "                stat1_val = stat1_val.to(dtype)[to_source_features]\n",
    "                stat2_val = target_feats.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "                stat2_val = stat2_val.to(dtype)[to_target_features]\n",
    "            else:\n",
    "                ValueError(f\"stat1 {stat} not recognized. Choose from: max, mean\")\n",
    "            logger.debug(f\"mean_vals (max): {stat1_val}\")\n",
    "            logger.debug(\"[9/9] Running SDXL with feature injection...\")\n",
    "            logger.debug(f\"Decoder weight shape: {sae.decoder.weight.shape}\")\n",
    "            logger.debug(f\"Using mode: {mode}\")\n",
    "            if mode == \"adding\":\n",
    "                # create the batch x feats x y x x tensor to add\n",
    "                delta = torch.zeros((1, 1280, 16, 16), device=device)\n",
    "                if maintain_spatial_info:\n",
    "                    delta[:, :, mask1] += m1*source.mean(dim=0, keepdim=True).to(delta.device) \n",
    "                    delta[:, :, mask1] -= m1*target.mean(dim=1, keepdim=True).to(delta.device)\n",
    "                else:\n",
    "                    print(target.shape)\n",
    "                    delta[:, :, mask2] += m1*source.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).to(delta.device)\n",
    "                    delta[:, :, mask2] -= m1*target.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).to(delta.device)\n",
    "                f = partial(add_activations, delta.to(dtype=source.dtype))\n",
    "                interventions[block] = f\n",
    "            elif mode == \"steering\":\n",
    "                # create the batch x feats x y x x tensor to add\n",
    "                delta = torch.zeros((1, 1280, 16, 16), dtype=source.dtype, device=device)\n",
    "                if subtract_target_add_source:\n",
    "                    if maintain_spatial_info:\n",
    "                        delta[:, :, mask1] += m1*source.mean(dim=0, keepdim=True).to(delta.device)\n",
    "                        delta[:, :, mask2] -= m1*target.mean(dim=0, keepdim=True).to(delta.device)\n",
    "                    else:\n",
    "                        delta[:, :, mask1] += m1*source.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).to(delta.device)\n",
    "                        delta[:, :, mask2] -= m1*target.mean(dim=0, keepdim=True).to(delta.device)\n",
    "                else:\n",
    "                    delta[:, :, mask2] += m1*source.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).to(delta.device)\n",
    "                    delta[:, :, mask2] -= m1*target.mean(dim=0, keepdim=True).to(delta.device)\n",
    "                f = partial(add_activations, delta)\n",
    "                interventions[block] = f\n",
    "            elif mode == \"sae_1\":\n",
    "                if subtract_target_add_source:       \n",
    "                    fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "                    if maintain_spatial_info:\n",
    "                        fmaps[:, mask1] += m1*source_feats.mean(dim=0, keepdim=True)[..., to_source_features].to(fmaps.device)\n",
    "                    else:\n",
    "                        fmaps[:, mask1] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                    f = partial(add_featuremaps, sae, to_source_features, to_target_features, m1, fmaps, mask2)\n",
    "                else:\n",
    "                    fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "                    fmaps[:, mask2] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                    f = partial(add_featuremaps, sae, to_source_features, to_target_features, m1, fmaps, mask2)\n",
    "                interventions[block] = f\n",
    "            elif mode == \"sae_eps\":\n",
    "                eps_source = torch.zeros((1, 1280, 16, 16), device=device, dtype=source.dtype)\n",
    "                eps_source[:, :, mask1] += source.mean(dim=0, keepdim=True).to(eps_source.device)\n",
    "                eps_source[:, :, mask1] -= (source_feats.to(device=eps_source.device) @ sae.decoder.weight.T).permute(0, 2, 1).mean(dim=0, keepdim=True).to(eps_source.device)    \n",
    "                if subtract_target_add_source:        \n",
    "                    fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "                    if maintain_spatial_info:\n",
    "                        fmaps[:, mask1] += m1*source_feats.mean(dim=0, keepdim=True)[..., to_source_features].to(fmaps.device)\n",
    "                    else:\n",
    "                        fmaps[:, mask1] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                        tmp = eps_source.mean(dim=2, keepdim=True).mean(dim=3, keepdim=False)\n",
    "                        eps_source -= eps_source\n",
    "                        eps_source[:, :, mask1] += tmp\n",
    "                    f = partial(add_featuremaps_and_eps, sae, to_source_features, to_target_features, m1, fmaps, mask2, eps_source=eps_source)\n",
    "                else:\n",
    "                    fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "                    fmaps[:, mask2] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                    tmp = eps_source.mean(dim=2, keepdim=True).mean(dim=3, keepdim=False)\n",
    "                    eps_source -= eps_source\n",
    "                    eps_source[:, :, mask2] += tmp\n",
    "                    f = partial(add_featuremaps_and_eps, sae, to_source_features, to_target_features, m1, fmaps, mask2, eps_source=eps_source)\n",
    "                interventions[block] = f\n",
    "            elif mode == \"sae_2\":\n",
    "                ValueError(\"deprecated\")\n",
    "                fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "                fmaps[:, mask2] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                f = partial(add_featuremaps_rescaling, sae, to_source_features, fmaps)\n",
    "                interventions[block] = f\n",
    "            elif mode == \"sae_ablation\":\n",
    "                ValueError(\"deprecated\")\n",
    "                fmaps = torch.zeros((1, 16, 16, len(to_target_features)), device=device)\n",
    "                fmaps[:, mask2] += (m1*stat2_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                f = partial(ablation, sae, to_target_features, fmaps)\n",
    "                interventions[block] = f\n",
    "            else:\n",
    "                ValueError(f\"Mode {mode} not recognized. Choose from: patch_max, patch_mean, sae_1, sae_2\")\n",
    "    else:\n",
    "        for shortcut in blocks_to_intervene:\n",
    "            for lidx in range(10):\n",
    "                layer = get_neuron_layer_name(shortcut, lidx)\n",
    "                # 1 x 39 x 5120\n",
    "                # set up interventions \n",
    "                to_source_neurons = to_source_features_dict[layer]\n",
    "                to_target_neurons = to_target_features_dict[layer]\n",
    "                source_neurons = source_feats_dict[layer]\n",
    "                target_neurons = target_feats_dict[layer]\n",
    "                print(\"source_neurons\", source_neurons.shape)\n",
    "                if verbose:\n",
    "                    plt.hist(source_neurons.flatten(), bins=\"rice\")\n",
    "                    plt.show()\n",
    "\n",
    "                if stat == \"quantile\":\n",
    "                    dtype = source_neurons.dtype\n",
    "                    stat1_val = source_neurons.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "                    stat1_val = stat1_val.to(dtype)[to_source_neurons]\n",
    "                    stat2_val = target_neurons.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "                    stat2_val = stat2_val.to(dtype)[to_target_neurons] \n",
    "                else:\n",
    "                    raise ValueError(f\"stat {stat} not supported\")\n",
    "\n",
    "                if subtract_target_add_source:        \n",
    "                    fmaps = torch.zeros((1, 16 * 16, 5120), device=device, dtype=source_neurons.dtype)\n",
    "                    if maintain_spatial_info:\n",
    "                        fmaps[:, mask1.flatten()][to_source_neurons] += m1*source_neurons.mean(dim=0, keepdim=True)[..., to_source_neurons].to(fmaps.device)\n",
    "                        fmaps[:, mask2.flatten()][to_target_neurons] -= m1*target_neurons.mean(dim=0, keepdim=True)[..., to_target_neurons].to(fmaps.device)\n",
    "                    else:\n",
    "                        #source_neurons torch.Size([4, 174, 5120])\n",
    "                        #stat1_val torch.Size([1])\n",
    "                        #fmaps torch.Size([1, 256, 5120])\n",
    "                        #test torch.Size([1, 174, 5120])\n",
    "                        #174\n",
    "                        #(256,)\n",
    "                        #nfeats 1\n",
    "                        print(\"stat1_val\",stat1_val.shape)\n",
    "                        print(\"fmaps\", fmaps.shape)\n",
    "                        print(\"test\", fmaps[:, mask1.flatten()].shape)\n",
    "                        print(mask1.sum())\n",
    "                        print(mask1.flatten().shape)\n",
    "                        print(\"nfeats\", len(to_source_neurons))\n",
    "                        print(\"tst\", fmaps[:, mask1.flatten()].shape)\n",
    "                        \n",
    "                        # Check if indices are valid before accessing\n",
    "                        if mask1.sum() > 0 and max(to_source_neurons) < fmaps.shape[2]:\n",
    "                            # Reshape stat1_val to match the expected dimensions\n",
    "                            # The error likely occurs because the dimensions don't match\n",
    "                            # or indices are out of bounds\n",
    "                            stat1_val_reshaped = (m1*stat1_val).view(1, 1, -1).to(fmaps.device)\n",
    "                            \n",
    "                            # Use scatter_ instead of direct indexing to avoid dimension issues\n",
    "                            for i, idx in enumerate(to_source_neurons):\n",
    "                                fmaps[:, mask1.flatten(), idx] += stat1_val_reshaped[:, :, i]\n",
    "                                \n",
    "                            # Similarly for target neurons\n",
    "                            target_mean = m1*target_neurons.mean(dim=0, keepdim=True)[..., to_target_neurons].to(fmaps.device)\n",
    "                            for i, idx in enumerate(to_target_neurons):\n",
    "                                fmaps[:, mask2.flatten(), idx] -= target_mean[:, :, i]\n",
    "                        else:\n",
    "                            print(\"Warning: Invalid indices detected. Skipping intervention.\")\n",
    "                    print(fmaps, (fmaps == 0).sum())\n",
    "                    print(fmaps.shape)\n",
    "                    f = partial(add_activations, fmaps)\n",
    "                else:\n",
    "                    fmaps = torch.zeros((1, 16 * 16, 5120), device=device)\n",
    "                    fmaps[:, mask2.flatten(), to_source_neurons] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "                    fmaps[:, mask2.flatten(), to_target_neurons] -= m1*target_neurons.mean(dim=0, keepdim=True)[..., to_target_neurons].to(fmaps.device)\n",
    "                    f = partial(add_activations, fmaps)\n",
    "                \n",
    "\n",
    "    result = pipe.run_with_hooks(\n",
    "        prompt2,\n",
    "        position_hook_dict=interventions,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed)\n",
    "    ).images[0]\n",
    "\n",
    "    # make a result figure that shows the images with masks and the intervened image\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Image 1 with mask from prompt 1\n",
    "    if \"~\" not in gsam_prompt1 and \"background\" not in gsam_prompt1 and gsam_prompt1 != \"#everything\":\n",
    "        axs[0].imshow(annotated_frame1)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt1 or \"~\" in gsam_prompt1:\n",
    "            # upsample mask by 32\n",
    "            mask1 = cv2.resize(mask1.astype(np.float32), (16*32, 16*32), interpolation=cv2.INTER_NEAREST)\n",
    "            # Plot the base image first\n",
    "            axs[0].imshow(img1)\n",
    "            # Then overlay the mask with higher alpha for visibility\n",
    "            axs[0].imshow(mask1, alpha=0.5)\n",
    "        else:\n",
    "            axs[0].imshow(img1)\n",
    "    axs[0].set_title(f\"{prompt1}\")\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    # Image 2 with mask from prompt 2\n",
    "    if \"~\" not in gsam_prompt2 and \"background\" not in gsam_prompt2 and gsam_prompt2 != \"#everything\":\n",
    "        axs[1].imshow(annotated_frame2)\n",
    "    else:\n",
    "        if \"background\" in gsam_prompt2 or \"~\" in gsam_prompt2:\n",
    "            # upsample mask by 32\n",
    "            mask2 = cv2.resize(mask2.astype(np.float32), (16*32, 16*32), interpolation=cv2.INTER_NEAREST)\n",
    "            # Plot the base image first\n",
    "            axs[1].imshow(img2)\n",
    "            # Then overlay the mask with higher alpha for visibility\n",
    "            axs[1].imshow(mask2, alpha=0.5)\n",
    "        else:\n",
    "            axs[1].imshow(img2)\n",
    "    axs[1].set_title(f\"{prompt2}\")\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    # Intervened result image\n",
    "    axs[2].imshow(result)\n",
    "    axs[2].axis('off')\n",
    "    # tight\n",
    "    #plt.tight_layout()\n",
    "    if result_name is not None:\n",
    "        plt.savefig(result_name + \"_summary.png\")\n",
    "        plt.close()\n",
    "        # save the images\n",
    "        with open(result_name + \"_feats_and_stats.json\", \"w\") as f:\n",
    "            json.dump({\"to_source_features\": {k:v.tolist() for k,v in to_source_features_dict.items()}, \n",
    "                       \"to_target_features\": {k:v.tolist() for k,v in to_target_features_dict.items()},\n",
    "                       \"m1\": m1,\n",
    "                       \"k_transfer\": k_transfer,\n",
    "                       \"stat\": stat,\n",
    "                       \"mode\": mode,\n",
    "                       \"combine_blocks\": combine_blocks,\n",
    "                       \"blocks_to_intervene\": blocks_to_intervene,\n",
    "                       }, f)\n",
    "        img1.save(result_name + f\"_{gsam_prompt2}_img1.png\")\n",
    "        img2.save(result_name + f\"_{gsam_prompt1}_img2.png\")\n",
    "        result.save(result_name + \".png\")\n",
    "    else:\n",
    "        print(to_source_features_dict)\n",
    "        print(to_target_features_dict)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a giraffe\", \"a photo of a colorful model\", \"giraffe\", \"face\", \n",
    "        blocks_to_intervene=[\"up.0.1\"], combine_blocks=True, subtract_target_add_source=False,\n",
    "        maintain_spatial_info=False,\n",
    "        n_steps=4, m1=10., k_transfer=10*5120, stat=\"quantile\", k=10, mode=\"neurons\", \n",
    "        result_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "from collections import defaultdict\n",
    "with open(\"./generated_piebench.json\", \"r\") as f:\n",
    "    pb = json.load(f)\n",
    "\n",
    "pb[0]\n",
    "blocks_to_intervene = []\n",
    "if use_down:\n",
    "     blocks_to_intervene.append(\"down.2.1\")\n",
    "if use_up:\n",
    "     blocks_to_intervene.append(\"up.0.1\")\n",
    "if use_up0:\n",
    "     blocks_to_intervene.append(\"up.0.0\")\n",
    "if use_mid:\n",
    "     blocks_to_intervene.append(\"mid.0\")\n",
    "\n",
    "\n",
    "expid2name= {\"0\":\"random\", # -> remove\n",
    "\"1\":\"change object\", # default\n",
    "\"2\":\"add object\", # done \n",
    "\"3\":\"delete object\", # done \n",
    "\"4\":\"change content\", # possible -> (this should apply to all edits i guess as long as the two versions of the image are similar to each other)take edit mask in original, boost editing mask and surpress orignal mask\n",
    "\"5\":\"change pose\", # somewhat possible -> switch\n",
    "\"6\":\"change color\", # done\n",
    "\"7\":\"change material\",# done \n",
    "\"8\":\"change background\", # done\n",
    "\"9\":\"change style\" # done }\n",
    "}\n",
    "\n",
    "def remove_brakets(txt):\n",
    "     return txt.replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "\n",
    "cnt = defaultdict(int)\n",
    "mode = \"steering\"\n",
    "m1 = 2.\n",
    "maintain_spatial_info = True\n",
    "prefix = os.path.join(prefix, \"steering\")\n",
    "for d in pb:\n",
    "     try:\n",
    "          if d[\"editing_type_id\"] in [] or cnt[d[\"editing_type_id\"]] > 10:\n",
    "               continue\n",
    "          key = d[\"id\"]\n",
    "          print(key)\n",
    "          path = os.path.join(prefix, f\"down{use_down}_up{use_up}_up0{use_up0}_mid{use_mid}_T{n_steps}_ktrans{k_transfer}_str{m1}/{d['editing_type_id']}\")\n",
    "          original_prompt = remove_brakets(d[\"original_prompt\"])\n",
    "          editing_prompt = remove_brakets(d[\"editing_prompt\"])\n",
    "          os.makedirs(path, exist_ok=True)\n",
    "          if d[\"editing_type_id\"] in ['0']:\n",
    "               continue\n",
    "          elif d[\"editing_type_id\"] in ['2']: # add object\n",
    "               main(editing_prompt, original_prompt, d[\"edit_target\"], d[\"edit_source\"], \n",
    "                    use_source_mask_in_both = True, subtract_target_add_source=True,\n",
    "                    maintain_spatial_info=maintain_spatial_info,\n",
    "                    blocks_to_intervene=blocks_to_intervene, combine_blocks=True,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"quantile\", k=10, mode=mode, \n",
    "                    result_name=f\"{path}/{key}\")\n",
    "          elif d[\"editing_type_id\"] in ['3']: # delete object\n",
    "               main(original_prompt, original_prompt, \"~\"+d[\"edit_source\"], d[\"edit_source\"], \n",
    "                    blocks_to_intervene=blocks_to_intervene, combine_blocks=True, subtract_target_add_source=False,\n",
    "                    maintain_spatial_info=maintain_spatial_info,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"quantile\", k=10, mode=mode, \n",
    "                    result_name=f\"{path}/{key}\")\n",
    "          elif d[\"editing_type_id\"] in ['4', '5'] + ['1', '6', '7', '8', '9']:\n",
    "               main(editing_prompt, original_prompt, d[\"edit_target\"], d[\"edit_source\"], \n",
    "                    blocks_to_intervene=blocks_to_intervene, combine_blocks=True, subtract_target_add_source=True,\n",
    "                    maintain_spatial_info=maintain_spatial_info,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"quantile\", k=10, mode=mode, \n",
    "                    result_name=f\"{path}/{key}\")\n",
    "          else:\n",
    "               main(editing_prompt, original_prompt, d[\"edit_target\"], d[\"edit_source\"], \n",
    "                    blocks_to_intervene=blocks_to_intervene, combine_blocks=True, maintain_spatial_info=False,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"quantile\", k=10, mode=mode, \n",
    "                    result_name=f\"{path}/{key}\")\n",
    "          cnt[d[\"editing_type_id\"]] += 1\n",
    "     except Exception as e:\n",
    "          print(e)\n",
    "          continue\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a giraffe\", \"a photo of a colorful model\", \"giraffe\", \"face\", \n",
    "        blocks_to_intervene=[\"up.0.1\"], combine_blocks=True, subtract_target_add_source=True,\n",
    "        maintain_spatial_info=False,\n",
    "        n_steps=4, m1=2., k_transfer=k_transfer, stat=\"quantile\", k=10, mode=\"adding\", \n",
    "        result_name=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxlsae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
