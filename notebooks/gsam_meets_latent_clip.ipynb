{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/tmp/wendler/hf_cache\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import argparse\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add necessary paths for custom modules\n",
    "sys.path.append(\"/share/u/wendler/code/my-sdxl-unbox\")\n",
    "\n",
    "from SDLens import HookedStableDiffusionXLPipeline\n",
    "from SAE import SparseAutoencoder\n",
    "from utils import add_feature_on_area_turbo\n",
    "\n",
    "import supervision as sv\n",
    "import pycocotools.mask as mask_util\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# Grounded SAM2 and Grounding DINO imports\n",
    "sys.path.append(\"/share/u/wendler/code/Grounded-SAM-2\")\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "from grounding_dino.groundingdino.util.inference import load_model, predict\n",
    "import grounding_dino.groundingdino.datasets.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "n_steps = 4\n",
    "m1 = 1.\n",
    "k_transfer = 5\n",
    "use_down = True\n",
    "use_up = True\n",
    "use_up0 = True\n",
    "use_mid = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_to_block = {\n",
    "        \"down.2.1\": \"unet.down_blocks.2.attentions.1\",\n",
    "        \"up.0.1\": \"unet.up_blocks.0.attentions.1\",\n",
    "        \"up.0.0\": \"unet.up_blocks.0.attentions.0\",\n",
    "        \"mid.0\": \"unet.mid_block.attentions.0\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SDLens/src to sys.path at the top of the scrip\n",
    "\n",
    "# --- Utility functions ---\n",
    "def resize_mask(mask, size=(16, 16)):\n",
    "    # consider all 32 by 32 windows in the mask\n",
    "    small = cv2.resize(mask.astype(np.float32), size, interpolation=cv2.INTER_LANCZOS4) > 0\n",
    "    if small.astype(np.float32).sum() == 0:\n",
    "        tmp = mask.reshape(16, 32, 16, 32).astype(np.float32)\n",
    "        tmp = tmp.sum(axis=1)\n",
    "        tmp = tmp.sum(axis=2)\n",
    "        if (tmp >= 32*32).astype(np.float32).sum() == 0:\n",
    "            print(\"trying to fix the mask...\")\n",
    "            # set the maximum gridcell to 1\n",
    "            amax = tmp.argmax()\n",
    "            tmp[np.unravel_index(amax, tmp.shape)] = 1\n",
    "            return tmp.astype(bool)\n",
    "    return small\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Tuple\n",
    "import grounding_dino.groundingdino.datasets.transforms as T\n",
    "\n",
    "def sam_mask(img, prompt, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD):\n",
    "    def load_image(img) -> Tuple[np.array, torch.Tensor]:\n",
    "        transform = T.Compose(\n",
    "            [\n",
    "                T.RandomResize([800], max_size=1333),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "        image_source = img.convert(\"RGB\")\n",
    "        image = np.asarray(image_source)\n",
    "        image_transformed, _ = transform(image_source, None)\n",
    "        return image, image_transformed\n",
    "    image_source, image = load_image(img)\n",
    "    sam2_predictor.set_image(image_source)\n",
    "\n",
    "    boxes, confidences, labels = predict(\n",
    "        model=grounding_model,\n",
    "        image=image,\n",
    "        caption=prompt,\n",
    "        box_threshold=BOX_THRESHOLD,\n",
    "        text_threshold=TEXT_THRESHOLD,\n",
    "    )\n",
    "\n",
    "    # process the box prompt for SAM 2\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    input_boxes = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "\n",
    "\n",
    "    # FIXME: figure how does this influence the G-DINO model\n",
    "    # torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "    #if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "        #torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        #torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    masks, scores, logits = sam2_predictor.predict(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        box=input_boxes,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Post-process the output of the model to get the masks, scores, and logits for visualization\n",
    "    \"\"\"\n",
    "    # convert the shape to (n, H, W)\n",
    "    if masks.ndim == 4:\n",
    "        masks = masks.squeeze(1)\n",
    "\n",
    "\n",
    "    confidences = confidences.numpy().tolist()\n",
    "    class_names = labels\n",
    "\n",
    "    class_ids = np.array(list(range(len(class_names))))\n",
    "\n",
    "    labels = [\n",
    "        f\"{class_name} {confidence:.2f}\"\n",
    "        for class_name, confidence\n",
    "        in zip(class_names, confidences)\n",
    "    ]\n",
    "\n",
    "    detections = sv.Detections(\n",
    "        xyxy=input_boxes,  # (n, 4)\n",
    "        mask=masks.astype(bool),  # (n, h, w)\n",
    "        class_id=class_ids\n",
    "    )\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator()\n",
    "    annotated_frame = box_annotator.annotate(scene=img.copy(), detections=detections)\n",
    "\n",
    "    label_annotator = sv.LabelAnnotator()\n",
    "    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n",
    "\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    annotated_frame = mask_annotator.annotate(scene=annotated_frame, detections=detections)\n",
    "    return detections, labels, annotated_frame\n",
    "    \n",
    "\n",
    "def best_features_saeuron(source_feats, target_feats, k=10):\n",
    "    mean_source = source_feats.mean(dim=0).mean(dim=0)\n",
    "    mean_target = target_feats.mean(dim=0).mean(dim=0)\n",
    "    scores = mean_source/mean_source.sum() - mean_target/mean_target.sum()\n",
    "    arg_sorted = np.argsort(scores.cpu().detach().numpy())\n",
    "    return arg_sorted[::-1][:k].copy(), arg_sorted[:k].copy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_features_per_block(cache1, cache2, mask1, mask2, k_transfer=None, blocks_to_intervene=None, saes=None):\n",
    "    print(blocks_to_intervene)\n",
    "    to_source_features_dict = {}\n",
    "    to_target_features_dict = {}\n",
    "    source_feats_dict = {}\n",
    "    target_feats_dict = {}\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    for shortcut in blocks_to_intervene:\n",
    "        block = code_to_block[shortcut]\n",
    "        diff1 = cache1['output'][block][0] - cache1['input'][block][0]\n",
    "        diff2 = cache2['output'][block][0] - cache2['input'][block][0]\n",
    "        source = diff1[:, :, mask1]\n",
    "        target = diff2[:, :, mask2]\n",
    "        sae = saes[shortcut]\n",
    "        source_feats = sae.encode(source.permute(0, 2, 1))\n",
    "        target_feats = sae.encode(target.permute(0, 2, 1))\n",
    "        to_source_features, to_target_features = best_features_saeuron(source_feats, target_feats, k=k_transfer)\n",
    "        to_source_features_dict[shortcut] = to_source_features\n",
    "        to_target_features_dict[shortcut] = to_target_features\n",
    "        source_feats_dict[shortcut] = source_feats.detach().cpu()\n",
    "        target_feats_dict[shortcut] = target_feats.detach().cpu()\n",
    "        source_dict[shortcut] = source.detach().cpu()\n",
    "        target_dict[shortcut] = target.detach().cpu()   \n",
    "    return to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict\n",
    "    \n",
    "@torch.no_grad()\n",
    "def get_features_all_blocks(cache1, cache2, mask1, mask2, k_transfer=None, blocks_to_intervene=None, saes=None):\n",
    "    print(\"processing blocks jointly...\")\n",
    "    source_feats_all = []\n",
    "    target_feats_all = []\n",
    "    to_source_feats_dict = {}\n",
    "    to_target_feats_dict = {}\n",
    "    source_feats_dict = {}\n",
    "    target_feats_dict = {}\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    block_sizes = {}\n",
    "    for shortcut in blocks_to_intervene:\n",
    "        block = code_to_block[shortcut]\n",
    "        diff1 = cache1['output'][block][0] - cache1['input'][block][0]\n",
    "        diff2 = cache2['output'][block][0] - cache2['input'][block][0]\n",
    "        source = diff1[:, :, mask1]\n",
    "        target = diff2[:, :, mask2]\n",
    "        sae = saes[shortcut]\n",
    "        source_feats = sae.encode(source.permute(0, 2, 1))\n",
    "        target_feats = sae.encode(target.permute(0, 2, 1))\n",
    "        source_feats_all.append(source_feats)\n",
    "        target_feats_all.append(target_feats)\n",
    "        source_dict[shortcut] = source.detach().cpu()\n",
    "        target_dict[shortcut] = target.detach().cpu()\n",
    "        source_feats_dict[shortcut] = source_feats.detach().cpu() \n",
    "        target_feats_dict[shortcut] = target_feats.detach().cpu()\n",
    "        block_sizes[shortcut] = source_feats.shape[-1]\n",
    "    source_feats_all = torch.cat(source_feats_all, dim=-1)\n",
    "    target_feats_all = torch.cat(target_feats_all, dim=-1)\n",
    "    to_source_feats_all, to_target_feats_all = best_features_saeuron(source_feats_all, target_feats_all, k=k_transfer)\n",
    "    start = 0\n",
    "    for idx, shortcut in enumerate(blocks_to_intervene):\n",
    "        to_source_feats = []\n",
    "        to_target_feats = []\n",
    "        for feat in to_source_feats_all:\n",
    "            if feat >= start and feat < start+block_sizes[shortcut]:\n",
    "                to_source_feats.append(feat-start)\n",
    "        for feat in to_target_feats_all:\n",
    "            if feat >= start and feat < start + block_sizes[shortcut]:\n",
    "                to_target_feats.append(feat-start)\n",
    "        to_source_feats_dict[shortcut] = np.asarray(to_source_feats)\n",
    "        to_target_feats_dict[shortcut] = np.asarray(to_target_feats)\n",
    "        start += block_sizes[shortcut]\n",
    "\n",
    "    return to_source_feats_dict, to_target_feats_dict, source_feats_dict, target_feats_dict, source_dict, target_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float16\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "pipe = HookedStableDiffusionXLPipeline.from_pretrained(\n",
    "    'stabilityai/sdxl-turbo',\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"balanced\",\n",
    "    variant=(\"fp16\" if dtype==torch.float16 else None)\n",
    ")\n",
    "pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM2_CHECKPOINT = \"/share/u/wendler/code/Grounded-SAM-2/checkpoints/sam2.1_hiera_large.pt\"\n",
    "SAM2_MODEL_CONFIG = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "GROUNDING_DINO_CONFIG = \"/share/u/wendler/code/Grounded-SAM-2/grounding_dino/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "GROUNDING_DINO_CHECKPOINT = \"/share/u/wendler/code/Grounded-SAM-2/gdino_checkpoints/groundingdino_swint_ogc.pth\"\n",
    "BOX_THRESHOLD = 0.35\n",
    "TEXT_THRESHOLD = 0.25\n",
    "sam2_model = build_sam2(SAM2_MODEL_CONFIG, SAM2_CHECKPOINT, device=device)\n",
    "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "grounding_model = load_model(\n",
    "    model_config_path=GROUNDING_DINO_CONFIG,\n",
    "    model_checkpoint_path=GROUNDING_DINO_CHECKPOINT,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "path_to_checkpoints = '/share/u/wendler/code/my-sdxl-unbox/checkpoints/'\n",
    "blocks = list(code_to_block.values())\n",
    "saes = {}\n",
    "k = 10\n",
    "exp = 4\n",
    "for shortcut in code_to_block.keys():\n",
    "    block = code_to_block[shortcut]\n",
    "    sae = SparseAutoencoder.load_from_disk(\n",
    "        os.path.join(path_to_checkpoints, f\"{block}_k{k}_hidden{exp*1280:d}_auxk256_bs4096_lr0.0001\", \"final\")\n",
    "    ).to(device, dtype=dtype)\n",
    "    saes[shortcut] = sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# --- Main experiment ---\n",
    "def add_featuremaps(sae, to_source_features, to_target_features, m1, fmaps, target_mask, module, input, output):\n",
    "        diff = output[0] - input[0]\n",
    "        coefs = sae.encode(diff.permute(0, 2, 3, 1))\n",
    "        mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "        mask[0,target_mask][..., to_target_features] -= m1*coefs[0, target_mask][..., to_target_features]\n",
    "        mask[..., to_source_features] += fmaps.to(mask.device)\n",
    "        to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "        return (output[0] + to_add.permute(0, 3, 1, 2).to(output[0].device),)\n",
    "\n",
    "def add_featuremaps_rescaling(sae, to_source_features, fmaps, module, input, output):\n",
    "    diff = output[0] - input[0]\n",
    "    mask = torch.zeros([fmaps.shape[0], fmaps.shape[1], fmaps.shape[2], sae.decoder.weight.shape[1]], device=input[0].device)\n",
    "    mask[..., to_source_features] += fmaps.to(mask.device)\n",
    "    to_add = mask.to(sae.decoder.weight.dtype) @ sae.decoder.weight.T\n",
    "    out = output[0]/output[0].norm(dim=1, keepdim=True)\n",
    "    out *= output[0].norm(dim=1, keepdim=True)  - to_add.permute(0, 3, 1, 2).norm(dim=1, keepdim=True)\n",
    "    out += to_add.permute(0, 3, 1, 2).to(output[0].device)\n",
    "    return (out, )\n",
    "\n",
    "def activation_patching(mean, target_mask, module, input, output):\n",
    "    diff = output[0] - input[0]\n",
    "    diff[0, :, target_mask] += mean[:, None]\n",
    "    return (diff + input[0],)\n",
    "\n",
    "\n",
    "def main(prompt1, prompt2, gsam_prompt1, gsam_prompt2, pipe=pipe, k=10, \n",
    "         blocks_to_intervene=[\"down.2.1\", \"up.0.1\", \"up.0.0\", \"mid.0\"],\n",
    "         n_steps=1, m1=2., k_transfer=10, stat=\"max\", mode=\"sae_1\", \n",
    "         combine_blocks=False,verbose=False,\n",
    "         sam_predictor=sam2_predictor, grounding_model=grounding_model, saes=saes, \n",
    "         result_name=None):\n",
    "    import logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "    if verbose:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        logger.setLevel(logging.INFO)    \n",
    "\n",
    "    logger.debug(\"[4/9] Generating images and caching activations...\")\n",
    "    seed = 42\n",
    "    base_imgs1, cache1 = pipe.run_with_cache(\n",
    "        prompt1,\n",
    "        positions_to_cache=blocks,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    base_imgs2, cache2 = pipe.run_with_cache(\n",
    "        prompt2,\n",
    "        positions_to_cache=blocks,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed),\n",
    "        save_input=True,\n",
    "    )\n",
    "    img1 = base_imgs1[0][0]\n",
    "    img2 = base_imgs2[0][0]\n",
    "\n",
    "    logger.debug(\"[5/9] Running Grounded SAM on generated images...\")\n",
    "    if gsam_prompt1 == \"background\":\n",
    "        mask1 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        detections1, labels1, annotated_frame1 = sam_mask(img1, gsam_prompt1, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "        masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections1.mask]\n",
    "        mask1 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if gsam_prompt2 == \"background\":\n",
    "        mask2 = np.ones((16, 16), dtype=bool)\n",
    "    else:\n",
    "        detections2, labels2, annotated_frame2 = sam_mask(img2, gsam_prompt2, sam2_predictor, grounding_model, BOX_THRESHOLD, TEXT_THRESHOLD)\n",
    "        masks = [resize_mask(bigmask).astype(np.float32) for bigmask in detections2.mask]\n",
    "        mask2 = np.stack(masks, axis=0).sum(axis=0).astype(bool)\n",
    "    if mask1.sum() == 0:\n",
    "        mask1 = np.ones((16, 16), dtype=bool)\n",
    "    if mask2.sum() == 0:\n",
    "        mask2 = np.ones((16, 16), dtype=bool)\n",
    "    if verbose:\n",
    "        plt.imshow(mask1)\n",
    "        plt.show()\n",
    "        plt.imshow(mask2)\n",
    "        plt.show()\n",
    "    logger.debug(\"[6/9] Extracting latents and encoding features...\")\n",
    "    interventions = {}\n",
    "    if combine_blocks:\n",
    "        to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict = \\\n",
    "            get_features_all_blocks(cache1, cache2, mask1, mask2, k_transfer=k_transfer, blocks_to_intervene=blocks_to_intervene, saes=saes)\n",
    "    else:\n",
    "        to_source_features_dict, to_target_features_dict, source_feats_dict, target_feats_dict, source_dict, target_dict = \\\n",
    "            get_features_per_block(cache1, cache2, mask1, mask2, k_transfer=k_transfer, blocks_to_intervene=blocks_to_intervene, saes=saes)\n",
    "    \n",
    "    \n",
    "    for shortcut in blocks_to_intervene:\n",
    "        # 1 x 39 x 5120\n",
    "        # set up interventions \n",
    "        to_source_features = to_source_features_dict[shortcut]\n",
    "        to_target_features = to_target_features_dict[shortcut]\n",
    "        source_feats = source_feats_dict[shortcut]\n",
    "        source = source_dict[shortcut]\n",
    "        target = target_dict[shortcut]\n",
    "        sae = saes[shortcut]\n",
    "        block = code_to_block[shortcut]\n",
    "        logger.debug(\"[7/9] Selecting best features...\")\n",
    "\n",
    "        # use max\n",
    "        if stat == \"max\":\n",
    "            stat1_val = source_feats.max(dim=0)[0].max(dim=0)[0][to_source_features]\n",
    "        elif stat == \"mean\":\n",
    "            logger.debug(f\"source_feats shape: {source_feats.shape}\")\n",
    "            stat1_val = source_feats[source_feats[:, :] > 1e-3].mean(dim=0)[to_source_features]\n",
    "            #for fidx in to_source_features:\n",
    "            #    coefs = source_feats[0][..., fidx]\n",
    "            #    mymeans.append(coefs[coefs > 1e-3].mean())\n",
    "            #stat1_val = torch.tensor(mymeans, device=torch.device(\"cuda\"))\n",
    "        elif stat == \"quantile\":\n",
    "            logger.debug(f\"source_feats shape: {source_feats.shape}\")\n",
    "            dtype = source_feats.dtype\n",
    "            stat1_val = source_feats.float().quantile(0.95, dim=1).mean(dim=0)\n",
    "            stat1_val = stat1_val.to(dtype)[to_source_features]\n",
    "        else:\n",
    "            ValueError(f\"stat1 {stat} not recognized. Choose from: max, mean\")\n",
    "        logger.debug(f\"mean_vals (max): {stat1_val}\")\n",
    "        logger.debug(\"[8/9] Preparing featuremaps for transfer...\")\n",
    "        fmaps = torch.zeros((1, 16, 16, len(to_source_features)), device=device)\n",
    "        fmaps[:, mask2] += (m1*stat1_val).unsqueeze(0).unsqueeze(0).to(fmaps.device)\n",
    "\n",
    "\n",
    "        logger.debug(\"[9/9] Running SDXL with feature injection...\")\n",
    "\n",
    "\n",
    "        logger.debug(f\"Decoder weight shape: {sae.decoder.weight.shape}\")\n",
    "        logger.debug(f\"Using mode: {mode}\")\n",
    "        if mode == \"patch_max\":\n",
    "            logger.debug(f\"Cat shape: {source.shape}\")\n",
    "            f = partial(activation_patching, m1*(source[0].max(dim=1)[0] - target[0].max(dim=1)[0]), mask2)\n",
    "            interventions[block] = f\n",
    "        elif mode == \"patch_mean\":\n",
    "            f = partial(activation_patching, m1*(source[0].mean(dim=1) - target[0].mean(dim=1)), mask2)\n",
    "            interventions[block] = f\n",
    "        elif mode == \"sae_1\":\n",
    "            f = partial(add_featuremaps, sae, to_source_features, to_target_features, m1, fmaps, mask2)\n",
    "            interventions[block] = f\n",
    "        elif mode == \"sae_2\":\n",
    "            f = partial(add_featuremaps_rescaling, sae, to_source_features, fmaps)\n",
    "            interventions[block] = f\n",
    "        else:\n",
    "            ValueError(f\"Mode {mode} not recognized. Choose from: patch_max, patch_mean, sae_1, sae_2\")\n",
    "\n",
    "    result = pipe.run_with_hooks(\n",
    "        prompt2,\n",
    "        position_hook_dict=interventions,\n",
    "        num_inference_steps=n_steps,\n",
    "        guidance_scale=0.0,\n",
    "        generator=torch.Generator(device='cpu').manual_seed(seed)\n",
    "    ).images[0]\n",
    "\n",
    "    # make a result figure that shows the images with masks and the intervened image\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Image 1 with mask from prompt 1\n",
    "    if gsam_prompt1 != \"background\":\n",
    "        axs[0].imshow(annotated_frame1)\n",
    "    else:\n",
    "        axs[0].imshow(img1)\n",
    "    axs[0].set_title(f\"{prompt1}\")\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    # Image 2 with mask from prompt 2\n",
    "    if gsam_prompt2 != \"background\":\n",
    "        axs[1].imshow(annotated_frame2)\n",
    "    else:\n",
    "        axs[1].imshow(img2)\n",
    "    axs[1].set_title(f\"{prompt2}\")\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    # Intervened result image\n",
    "    axs[2].imshow(result)\n",
    "    axs[2].axis('off')\n",
    "    # tight\n",
    "    #plt.tight_layout()\n",
    "    if result_name is not None:\n",
    "        plt.savefig(result_name + \"_summary.png\")\n",
    "        plt.close()\n",
    "        # save the images\n",
    "        img1.save(result_name + f\"_{gsam_prompt2}_img1.png\")\n",
    "        img2.save(result_name + f\"_{gsam_prompt1}_img2.png\")\n",
    "        result.save(result_name + \".png\")\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from collections import defaultdict\n",
    "with open(\"/share/u/wendler/PIE-Bench_v1/mapping_file.json\", \"r\") as f:\n",
    "    pb = json.load(f)\n",
    "print(list(pb.keys()))\n",
    "print(list(pb[\"000000000001\"].keys()))\n",
    "print(pb[\"000000000001\"])\n",
    "\n",
    "cnts = defaultdict(int)\n",
    "for key, val in pb.items():\n",
    "    cnts[val[\"editing_type_id\"]]+=1\n",
    "    #print(val[\"blended_word\"])\n",
    "print(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a giraffe\", \"2 people\", \"giraffe\", \"person\", \n",
    "                    blocks_to_intervene=[\"down.2.1\"],\n",
    "                    n_steps=1, m1=1, k_transfer=3, stat=\"quantile\", k=10, mode=\"sae_1\", \n",
    "                    result_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a giraffe\", \"a photo of a colorful model\", \"giraffe\", \"face\", \n",
    "                    blocks_to_intervene=[\"up.0.1\"],#.0.1\"],\n",
    "                    n_steps=4, m1=1.5, k_transfer=1, stat=\"quantile\", k=10, mode=\"sae_1\", \n",
    "                    result_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a giraffe\", \"a photo of a colorful model\", \"giraffe\", \"face\", \n",
    "                    blocks_to_intervene=[\"up.0.1\"],#.0.1\"],\n",
    "                    n_steps=4, m1=5.5, k_transfer=10, stat=\"quantile\", k=10, mode=\"sae_1\", \n",
    "                    result_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a white raven sitting on a branch\", \"a black raven sitting on a branch\", \"white raven\", \"black raven\", \n",
    "                    blocks_to_intervene=[\"up.0.1\"],\n",
    "                    n_steps=1, m1=1., k_transfer=1, stat=\"quantile\", k=10, mode=\"sae_1\", \n",
    "                    result_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a white raven sitting on a branch\", \"a black raven sitting on a branch\", \"white raven\", \"black raven\", \n",
    "                    blocks_to_intervene=[\"up.0.1\"],\n",
    "                    n_steps=1, m1=3., k_transfer=1, stat=\"quantile\", k=10, mode=\"sae_2\", \n",
    "                    result_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a face  with two open eyes\", \"a face with two closed eyes\", \"open eye\", \"closed eye\", \n",
    "                    blocks_to_intervene=[\"up.0.0\", \"down.2.1\"],# \"up.0.0\"],\n",
    "                    n_steps=4, m1=5., k_transfer=3, stat=\"quantile\", k=10, mode=\"sae_2\", \n",
    "                    result_name=None, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a face  with two open eyes\", \"a face with two closed eyes\", \"open eye\", \"closed eye\", \n",
    "                    blocks_to_intervene=[\"up.0.0\", \"down.2.1\"],# \"up.0.0\"],\n",
    "                    n_steps=4, m1=1.5, k_transfer=5, stat=\"quantile\", k=10, mode=\"sae_1\", \n",
    "                    result_name=None, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a man playing a guitar\", \"a man holding a fish\", \"guitar\", \"fish\", \n",
    "    blocks_to_intervene=[\"up.0.0\", \"down.2.1\"],# \"up.0.0\"],\n",
    "    n_steps=4, m1=2., k_transfer=10, stat=\"quantile\", k=10, mode=\"sae_1\", \n",
    "    combine_blocks=True,\n",
    "    result_name=None, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a man playing a guitar\", \"a man holding a fish\", \"guitar\", \"fish\", \n",
    "    blocks_to_intervene=[\"up.0.0\", \"down.2.1\", \"mid.0\", \"up.0.1\"],# \"up.0.0\"],\n",
    "    n_steps=4, m1=2., k_transfer=10, stat=\"quantile\", k=10, mode=\"sae_1\", \n",
    "    combine_blocks=True,\n",
    "    result_name=None, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"a face  with two open eyes\", \"a face with two closed eyes\", \"open eye\", \"closed eye\", \n",
    "                    blocks_to_intervene=[\"up.0.0\", \"down.2.1\", \"mid.0\", \"up.0.1\"],\n",
    "                    n_steps=4, m1=1.5, k_transfer=10, stat=\"quantile\", k=10, mode=\"sae_1\", \n",
    "                    combine_blocks=True, result_name=None, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "blocks_to_intervene = []\n",
    "if use_down:\n",
    "     blocks_to_intervene.append(\"down.2.1\")\n",
    "if use_up:\n",
    "     blocks_to_intervene.append(\"up.0.1\")\n",
    "if use_up0:\n",
    "     blocks_to_intervene.append(\"up.0.0\")\n",
    "if use_mid:\n",
    "     blocks_to_intervene.append(\"mid.0\")\n",
    "\n",
    "\n",
    "for key, val in pb.items():\n",
    "     if True or (int(val[\"editing_type_id\"]) >= 0 and cnts[val[\"editing_type_id\"]] < 10):\n",
    "          try:\n",
    "               if val[\"blended_word\"] == \"\":\n",
    "                    continue\n",
    "               w1 = val[\"blended_word\"].split(\" \")[0]\n",
    "               w2 = val[\"blended_word\"].split(\" \")[1]\n",
    "               os.makedirs(f\"../results/PIE-Bench/{n_steps}_{m1}_{k_transfer}_{use_down}_{use_up}_{use_up0}_{use_mid}/{val['editing_type_id']}\", exist_ok=True)\n",
    "               main(val[\"editing_prompt\"], val[\"original_prompt\"], w2, w1, \n",
    "                    blocks_to_intervene=blocks_to_intervene,\n",
    "                    n_steps=n_steps, m1=m1, k_transfer=k_transfer, stat=\"max\", k=10, mode=\"sae\", \n",
    "                    result_name=f\"../results/PIE-Bench/{n_steps}_{m1}_{k_transfer}_{use_down}_{use_up}_{use_up0}_{use_mid}/{val['editing_type_id']}/{key}\")\n",
    "               cnts[val[\"editing_type_id\"]]+=1\n",
    "          except Exception as e:\n",
    "               print(e)\n",
    "               continue\n",
    "          "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxlsae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
