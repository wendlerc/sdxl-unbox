{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"NewtonRaphsonInversion\")\n",
    "\n",
    "import torch\n",
    "from src.config import RunConfig\n",
    "from ipywidgets import Text, VBox\n",
    "import PIL\n",
    "from src.euler_scheduler import MyEulerAncestralDiscreteScheduler\n",
    "from diffusers.pipelines.auto_pipeline import AutoPipelineForImage2Image\n",
    "from src.sdxl_inversion_pipeline import SDXLDDIMPipeline\n",
    "from PIL import Image\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inversion_callback(pipe, step, timestep, callback_kwargs):\n",
    "    return callback_kwargs\n",
    "\n",
    "\n",
    "def inference_callback(pipe, step, timestep, callback_kwargs):\n",
    "    return callback_kwargs\n",
    "\n",
    "def center_crop(im):\n",
    "    width, height = im.size  # Get dimensions\n",
    "    min_dim = min(width, height)\n",
    "    left = (width - min_dim) / 2\n",
    "    top = (height - min_dim) / 2\n",
    "    right = (width + min_dim) / 2\n",
    "    bottom = (height + min_dim) / 2\n",
    "\n",
    "    # Crop the center of the image\n",
    "    im = im.crop((left, top, right, bottom))\n",
    "    return im\n",
    "\n",
    "\n",
    "def load_im_into_format_from_path(im_path, size=(1024, 1024)):\n",
    "    return center_crop(PIL.Image.open(im_path)).resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "model = \"stabilityai/sdxl-turbo\"\n",
    "\n",
    "class ImageEditorDemo:\n",
    "    def __init__(self, pipe_inversion, pipe_inference, input_image, description_prompt, cfg, edit_cfg=1.2):\n",
    "        self.pipe_inversion = pipe_inversion\n",
    "        self.pipe_inference = pipe_inference\n",
    "        self.load_image = True\n",
    "        g_cpu = torch.Generator().manual_seed(7865)\n",
    "        if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "            img_size = (1024,1024)\n",
    "        else:\n",
    "            img_size = (512,512)\n",
    "        self.original_image = load_im_into_format_from_path(input_image, img_size).convert(\"RGB\")\n",
    "\n",
    "        # resise input image\n",
    "        VQAE_SCALE = 8\n",
    "        latents_size = (1, 4, img_size[0] // VQAE_SCALE, img_size[1] // VQAE_SCALE)\n",
    "        print(pipe_inversion.unet.dtype)\n",
    "        noise = [randn_tensor(latents_size, dtype=pipe_inversion.unet.dtype, device=torch.device(\"cuda:0\"), generator=g_cpu) for i\n",
    "                 in range(cfg.num_inversion_steps)]\n",
    "        print(noise[0].shape)\n",
    "        pipe_inversion.scheduler.set_noise_list(noise)\n",
    "        pipe_inference.scheduler.set_noise_list(noise)\n",
    "        pipe_inversion.scheduler_inference.set_noise_list(noise)\n",
    "        pipe_inversion.set_progress_bar_config(disable=True)\n",
    "        pipe_inference.set_progress_bar_config(disable=True)\n",
    "        self.cfg = cfg\n",
    "        self.pipe_inversion.cfg = cfg\n",
    "        self.pipe_inference.cfg = cfg\n",
    "        self.inv_hp = [2, 0.1, 0.2] # niter, alpha, lr 2, 0.1, 0.2 is default\n",
    "        self.edit_cfg = edit_cfg\n",
    "\n",
    "        #self.pipe_inference.to(\"cuda\")\n",
    "        #self.pipe_inversion.to(\"cuda\")\n",
    "\n",
    "        self.last_latent = self.invert(self.original_image, description_prompt)\n",
    "        self.original_latent = self.last_latent\n",
    "\n",
    "    def invert(self, init_image, base_prompt):\n",
    "        res = self.pipe_inversion(prompt=base_prompt,\n",
    "                             num_inversion_steps=self.cfg.num_inversion_steps,\n",
    "                             num_inference_steps=self.cfg.num_inference_steps,\n",
    "                             image=init_image,\n",
    "                             guidance_scale=self.cfg.guidance_scale,\n",
    "                             callback_on_step_end=inversion_callback,\n",
    "                             strength=self.cfg.inversion_max_step,\n",
    "                             denoising_start=1.0 - self.cfg.inversion_max_step,\n",
    "                             inv_hp=self.inv_hp)[0][0]\n",
    "        return res\n",
    "\n",
    "    def edit(self, target_prompt, guidance_scale=None):\n",
    "        if guidance_scale is None:\n",
    "            guidance_scale = self.edit_cfg\n",
    "        image = self.pipe_inference(prompt=target_prompt,\n",
    "                            num_inference_steps=self.cfg.num_inference_steps,\n",
    "                            negative_prompt=\"\",\n",
    "                            callback_on_step_end=inference_callback,\n",
    "                            image=self.last_latent,\n",
    "                            strength=self.cfg.inversion_max_step,\n",
    "                            denoising_start=1.0 - self.cfg.inversion_max_step,\n",
    "                            guidance_scale=guidance_scale).images[0]\n",
    "        return image.resize((512, 512))\n",
    "    \n",
    "    def edit_with_hooks(self, target_prompt, position_hook_dict, guidance_scale=None):\n",
    "        if guidance_scale is None:\n",
    "            guidance_scale = self.edit_cfg\n",
    "        image = self.pipe_inference.run_with_hooks(prompt=target_prompt,\n",
    "                            position_hook_dict=position_hook_dict,\n",
    "                            num_inference_steps=self.cfg.num_inference_steps,\n",
    "                            negative_prompt=\"\",\n",
    "                            callback_on_step_end=inference_callback,\n",
    "                            image=self.last_latent,\n",
    "                            strength=self.cfg.inversion_max_step,\n",
    "                            denoising_start=1.0 - self.cfg.inversion_max_step,\n",
    "                            guidance_scale=guidance_scale).images[0]\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SDLens import HookedStableDiffusionXLImg2ImgPipeline\n",
    "from utils import add_feature, add_feature_on_area_turbo, add_feature_on_area_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ[\"HF_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "    image_size = (1024,1024)\n",
    "else:\n",
    "    image_size = (512,512)\n",
    "dtype = torch.float32\n",
    "scheduler_class = MyEulerAncestralDiscreteScheduler\n",
    "if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "    pipe_inversion = SDXLDDIMPipeline.from_pretrained(model, \n",
    "                                                      torch_dtype=dtype,\n",
    "                                                      device_map=\"balanced\",\n",
    "                                                      variant=(\"fp16\" if dtype==torch.float16 else None),\n",
    "                                                      cache_dir=os.path.join(os.environ[\"HF_HOME\"], \"tmp\")\n",
    "    )\n",
    "    pipe_inference = HookedStableDiffusionXLImg2ImgPipeline.from_pretrained(model, \n",
    "                                                                        torch_dtype=dtype,\n",
    "                                                                        device_map=\"balanced\",\n",
    "                                                                        variant=(\"fp16\" if dtype==torch.float16 else None),\n",
    "                                                                        cache_dir=os.path.join(os.environ[\"HF_HOME\"], \"tmp\")\n",
    "                                                                    )\n",
    "    if dtype == torch.float32:\n",
    "        pipe_inversion.text_encoder_2.to(dtype)\n",
    "        pipe_inference.text_encoder_2.to(dtype)\n",
    "else:\n",
    "    pipe_inversion = SDXLDDIMPipeline.from_pretrained(model, use_safetensors=True, safety_checker=None, cache_dir=os.environ[\"HF_HOME\"]).to(device)\n",
    "    pipe_inference = HookedStableDiffusionXLImg2ImgPipeline.from_pretrained(model, use_safetensors=True, safety_checker=None, cache_dir=os.environ[\"HF_HOME\"]).to(device)\n",
    "\n",
    "#pipe_inference = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/sdxl-turbo\", use_safetensors=True, safety_checker= None, cache_dir=os.environ[\"HF_HOME\"]).to(device)\n",
    "pipe_inference.scheduler            = scheduler_class.from_config(pipe_inference.scheduler.config)\n",
    "pipe_inversion.scheduler            = scheduler_class.from_config(pipe_inversion.scheduler.config)\n",
    "pipe_inversion.scheduler_inference  = scheduler_class.from_config(pipe_inference.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "    config = RunConfig(num_inference_steps=20,\n",
    "                   num_inversion_steps=20,\n",
    "                   guidance_scale=0.0,\n",
    "                   inversion_max_step=0.6) #4,4,0,0.6 is default settings 0.6 and 0.7 look the same\n",
    "else:\n",
    "    config = RunConfig(num_inference_steps=4,\n",
    "                   num_inversion_steps=4,\n",
    "                   guidance_scale=0.0,\n",
    "                   inversion_max_step=0.6) #4,4,0,0.6 is default settings 0.6 and 0.7 look the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_inversion.text_encoder_2.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up SAE intervention\n",
    "from SAE import SparseAutoencoder\n",
    "dtype = torch.float32\n",
    "\n",
    "path_to_checkpoints = './checkpoints/'\n",
    "\n",
    "code_to_block = {\n",
    "    \"down.2.1\": \"unet.down_blocks.2.attentions.1\",\n",
    "    \"mid.0\": \"unet.mid_block.attentions.0\",\n",
    "    \"up.0.1\": \"unet.up_blocks.0.attentions.1\",\n",
    "    \"up.0.0\": \"unet.up_blocks.0.attentions.0\"\n",
    "}\n",
    "\n",
    "saes_dict = {}\n",
    "means_dict = {}\n",
    "\n",
    "for code, block in code_to_block.items():\n",
    "    sae = SparseAutoencoder.load_from_disk(\n",
    "        os.path.join(path_to_checkpoints, f\"{block}_k10_hidden5120_auxk256_bs4096_lr0.0001\", \"final\"),\n",
    "    )\n",
    "    means = torch.load(\n",
    "        os.path.join(path_to_checkpoints, f\"{block}_k10_hidden5120_auxk256_bs4096_lr0.0001\", \"final\", \"mean.pt\"),\n",
    "        weights_only=True\n",
    "    )\n",
    "    saes_dict[code] = sae.to('cuda', dtype=dtype)\n",
    "    means_dict[code] = means.to('cuda', dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_dict[\"down.2.1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# back to inversion notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = display(display_id='my-display')\n",
    "input_image = \"resourses/chris.png\"\n",
    "description_prompt = 'a guy standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background'\n",
    "editor = ImageEditorDemo(pipe_inversion, pipe_inference, input_image, description_prompt, config, edit_cfg=1.2) #1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "text_input = widgets.Text(\n",
    "        value=description_prompt,\n",
    "        description=\"Prompt:\",\n",
    "        style={\"description_width\": \"initial\"},\n",
    "        layout=widgets.Layout(width='70%'), \n",
    "    )\n",
    "\n",
    "def f(x):\n",
    "    h.update(editor.edit(text_input.value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.display(editor.edit(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_hook_dict = {\"unet.down_blocks.2.attentions.1\": \n",
    "                      lambda *args, **kwargs: add_feature(\n",
    "                                saes_dict[\"down.2.1\"],\n",
    "                                2301,\n",
    "                                10*means_dict[\"down.2.1\"].mean(),\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_hook_dict = {\"unet.down_blocks.2.attentions.1\": \n",
    "                      lambda *args, **kwargs: add_feature(\n",
    "                                saes_dict[\"down.2.1\"],\n",
    "                                2301,\n",
    "                                -10*means_dict[\"down.2.1\"].mean(),\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_hook_dict = {\"unet.down_blocks.2.attentions.1\": \n",
    "                      lambda *args, **kwargs: add_feature(\n",
    "                                saes_dict[\"down.2.1\"],\n",
    "                                4998,\n",
    "                                10*means_dict[\"down.2.1\"].mean(),\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_hook_dict = {\"unet.down_blocks.2.attentions.1\": \n",
    "                      lambda *args, **kwargs: add_feature(\n",
    "                                saes_dict[\"down.2.1\"],\n",
    "                                4998,\n",
    "                                -10*means_dict[\"down.2.1\"].mean(),\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_hook_dict = {code_to_block[\"up.0.1\"]: \n",
    "                      lambda *args, **kwargs: add_feature(\n",
    "                                saes_dict[\"up.0.1\"],\n",
    "                                90,\n",
    "                                15*means_dict[\"up.0.1\"].mean(),\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_hook_dict = {code_to_block[\"up.0.1\"]: \n",
    "                      lambda *args, **kwargs: add_feature(\n",
    "                                saes_dict[\"up.0.1\"],\n",
    "                                90,\n",
    "                                0*means_dict[\"up.0.1\"].mean(),\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_hook_dict = {code_to_block[\"up.0.0\"]: \n",
    "                      lambda *args, **kwargs: add_feature(\n",
    "                                saes_dict[\"up.0.0\"],\n",
    "                                4594,\n",
    "                                -8*means_dict[\"up.0.0\"].mean(),\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_hook_dict = {code_to_block[\"up.0.0\"]: \n",
    "                      lambda *args, **kwargs: add_feature(\n",
    "                                saes_dict[\"up.0.0\"],\n",
    "                                4594,\n",
    "                                8*means_dict[\"up.0.0\"].mean(),\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a 16x16 mask\n",
    "# 90 fur, 3718 giraffe\n",
    "\n",
    "def myhook(module, inputs, outputs):\n",
    "    print(inputs[0].shape)\n",
    "    print(outputs[0].shape)\n",
    "    return outputs\n",
    "\n",
    "if model == \"stabilityai/stable-diffusion-xl-base-1.0\":\n",
    "    mask = torch.zeros((1, 1, 32, 32))\n",
    "    mask[:, :, 6:, 8:24] = 1\n",
    "else:\n",
    "    mask = torch.zeros((1, 1, 16, 16))\n",
    "    mask[:, :, 3:, 4:12] = 1\n",
    "\n",
    "block = \"up.0.1\"\n",
    "fidx = 90\n",
    "strength = 12\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_turbo(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs),\n",
    "                        code_to_block[\"up.0.0\"]: myhook}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a 16x16 mask\n",
    "# up.0.1: 90 fur, 3718 giraffe, 4977 tiger, 1393 leopard, 4197 green\n",
    "\n",
    "def myhook(module, inputs, outputs):\n",
    "    print(inputs[0].shape)\n",
    "    print(outputs[0].shape)\n",
    "    return outputs\n",
    "\n",
    "\n",
    "mask = torch.zeros((1, 1, 16, 16))\n",
    "#mask[:, :, 3:, 4:12] = 1\n",
    "mask[:, :, 10:13, 5:11] = 1\n",
    "block = \"up.0.0\"\n",
    "fidx = 4594 # moustache feature # 3742 beard?\n",
    "strength = 10\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_turbo(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_base(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict))\n",
    "\n",
    "h.display(editor.edit(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a 16x16 mask\n",
    "# up.0.1: 90 fur, 3718 giraffe, 4977 tiger, 1393 leopard, 4197 green\n",
    "\n",
    "mask = torch.zeros((1, 1, 16, 16))\n",
    "#mask[:, :, 3:, 4:12] = 1\n",
    "mask[:, :, 7:9, 4:12] = 1\n",
    "block = \"up.0.0\"\n",
    "fidx = 2638 # sunglasses # 3742 beard?\n",
    "strength = 10\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_turbo(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_base(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy wearing standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background.\",\n",
    "                                 position_hook_dict))\n",
    "\n",
    "h.display(editor.edit(\"a guy with dark black sunglasses standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a 16x16 mask\n",
    "# up.0.1: 90 fur, 3718 giraffe, 4977 tiger, 1393 leopard, 4197 green\n",
    "\n",
    "mask = torch.zeros((1, 1, 16, 16))\n",
    "#mask[:, :, 3:, 4:12] = 1\n",
    "mask[:, :, 10:14, 5:11] = 1\n",
    "block = \"up.0.0\"\n",
    "fidx = 2937 # 4161 # smile # 5048 # sad face # 2937 # shouting # 3742 beard?\n",
    "strength = 10\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_turbo(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_base(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy while standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict))\n",
    "\n",
    "h.display(editor.edit(\"a guy shouting at the camera while standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "          guidance_scale=1.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a 16x16 mask\n",
    "# up.0.1: 90 fur, 3718 giraffe, 4977 tiger, 1393 leopard, 4197 green\n",
    "\n",
    "mask = torch.zeros((1, 1, 16, 16))\n",
    "#mask[:, :, 3:, 4:12] = 1\n",
    "mask[:, :, 6:8, 4:12] = 1\n",
    "block = \"up.0.1\"\n",
    "fidx = 90 \n",
    "strength = 15\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_turbo(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_base(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy wearing standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background.\",\n",
    "                                 position_hook_dict))\n",
    "\n",
    "h.display(editor.edit(\"a guy with dark black sunglasses standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a 16x16 mask\n",
    "# 90 fur, 3718 giraffe, 4977 tiger, 1393 leopard\n",
    "\n",
    "mask = torch.zeros((1, 1, 16, 16))\n",
    "mask[:, :, 3:, 4:12] = 1\n",
    "block = \"down.2.1\"\n",
    "fidx = 527# 2301 evil # 89 muscular # 4074 anime # 179 horse\n",
    "strength = 25\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_turbo(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict,\n",
    "                                 guidance_scale=0.0)) # this type of intervention should be adapted before using it with guidance scale\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_base(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict))\n",
    "\n",
    "h.display(editor.edit(\"a black guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a 16x16 mask\n",
    "# 90 fur, 3718 giraffe, 4977 tiger, 1393 leopard\n",
    "\n",
    "mask = torch.zeros((1, 1, 16, 16))\n",
    "mask[:, :, 3:, 4:12] = 1\n",
    "block = \"down.2.1\"\n",
    "fidx = 349 # kid # 527 # black # 2301 evil # 89 muscular # 4074 anime # 179 horse\n",
    "strength = 9\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_turbo(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict))\n",
    "\n",
    "position_hook_dict = {code_to_block[block]: \n",
    "                      lambda *args, **kwargs: add_feature_on_area_base(\n",
    "                                saes_dict[block],\n",
    "                                fidx,\n",
    "                                mask.cuda() * strength * means_dict[block][fidx],\n",
    "                                *args, **kwargs)}\n",
    "h.display(editor.edit_with_hooks(\"a guy with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\",\n",
    "                                 position_hook_dict))\n",
    "\n",
    "h.display(editor.edit(\"a photo of a kid with a mustache standing on a mountain in Hokaidoo with brown hair and a blue hiking shirt, there is a city in the background\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdxlsae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
